---
title: "ä½¿ç”¨ Hugging Face è½»æ¾æ„å»ºå’Œå…±äº« ROCm å†…æ ¸"
thumbnail: /blog/assets/build-rocm-kernels/thumbnail.png
authors:
- user: badaoui
- user: daniehua
- user: ColorsWind
- user: ftyghome
translators:
- user: chenglu
---

# ä½¿ç”¨ Hugging Face è½»æ¾æ„å»ºå¹¶åˆ†äº« ROCm å†…æ ¸

![Easily Build and Share ROCm Kernels with Hugging Face](/blog/assets/build-rocm-kernels/thumbnail.png)

## ç®€ä»‹

è‡ªå®šä¹‰å†…æ ¸æ˜¯é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå®ƒè®© GPU æ“ä½œèƒ½å®Œå…¨è´´åˆä½ çš„å·¥ä½œè´Ÿè½½éœ€æ±‚â€”â€”æ— è®ºæ˜¯å›¾åƒå¤„ç†ã€å¼ é‡å˜æ¢ï¼Œè¿˜æ˜¯å…¶ä»–è®¡ç®—å¯†é›†å‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¦ä¸ºæ­£ç¡®çš„æ¶æ„ç¼–è¯‘è¿™äº›å†…æ ¸ã€é…ç½®å„ç§ç¼–è¯‘æ ‡å¿—å¹¶å¹²å‡€åœ°æ•´åˆåˆ° PyTorch æ‰©å±•ä¸­ï¼Œå¾€å¾€ä¼šå˜æˆä¸€å›¢ä¹±éº»ï¼ˆCMake/Nixã€ç¼–è¯‘é”™è¯¯ã€ABI é—®é¢˜ç­‰ï¼‰ã€‚
Hugging Face æä¾›çš„ [**kernel-builder**](https://github.com/huggingface/kernel-builder) å’Œ [**kernels**](https://github.com/huggingface/kernels) åº“ï¼Œè®©ä½ èƒ½è½»æ¾åœ°åœ¨ [**kernels-community**](https://huggingface.co/kernels-community) ä¸Šåˆ†äº«è¿™äº›è‡ªå®šä¹‰å†…æ ¸ï¼Œæ”¯æŒå¤šç§ GPU å’ŒåŠ é€Ÿå™¨åç«¯ï¼ŒåŒ…æ‹¬ CUDAã€ROCmã€Metal å’Œ XPUã€‚è¿™ä¿è¯äº†ä½ çš„å†…æ ¸æ—¢é«˜æ•ˆã€åˆå¯ç§»æ¤ï¼Œè¿˜èƒ½æ— ç¼é›†æˆåˆ° PyTorch ä¸­ã€‚

æœ¬æ–‡ä¸“æ³¨äºæ„å»º **ROCm å…¼å®¹å†…æ ¸**ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ [kernel-builder](https://github.com/huggingface/kernel-builder/tree/main) è¿›è¡Œæ„å»ºã€æµ‹è¯•ä¸åˆ†äº«ã€‚ä½ å°†å­¦ä¹ å¦‚ä½•åœ¨ AMD GPU ä¸Šé«˜æ•ˆè¿è¡Œè‡ªå®šä¹‰å†…æ ¸ï¼Œä»¥åŠå¯å¤ç°æ€§ã€æ‰“åŒ…ä¸éƒ¨ç½²çš„æœ€ä½³å®è·µã€‚

æœ¬æ–‡æ˜¯é’ˆå¯¹ ROCm çš„ç®€åŒ–ç‰ˆæ•™ç¨‹ã€‚å¦‚æœä½ æƒ³äº†è§£ CUDA ç›¸å…³å†…å®¹ï¼Œå¯å‚é˜…åŸæ–‡ï¼š[A Guide to Building and Scaling Production-Ready CUDA Kernels](https://huggingface.co/blog/kernel-builder)ã€‚

## æ„å»ºæ­¥éª¤

æˆ‘ä»¬ä»¥ [RadeonFlow_Kernels](https://github.com/RadeonFlow/RadeonFlow_Kernels) ä¸­çš„ GEMM å†…æ ¸ä¸ºä¾‹ã€‚è‹¥æƒ³ç›´æ¥æŸ¥çœ‹æ•™ç¨‹ï¼Œå¯[ç‚¹å‡»æ­¤å¤„](#step-1-project-structure)ã€‚

### å…³äºè¿™ä¸ªå†…æ ¸

> [!NOTE]
> æœ¬èŠ‚ç”± **RadeonFlow GEMM** å†…æ ¸ä½œè€…æ’°å†™ã€‚
> ä½œè€…ï¼š[ColorsWind](https://huggingface.co/ColorsWind)ã€[Zesen Liu](https://huggingface.co/ftyghome)ã€[Andy](https://huggingface.co/jpy794)

**RadeonFlow GEMM** å†…æ ¸æ˜¯ä¸€ä¸ªé’ˆå¯¹ AMD Instinct MI300X GPU ä¼˜åŒ–çš„é«˜æ€§èƒ½ FP8 åˆ†å—çŸ©é˜µä¹˜æ³•å®ç°ã€‚
GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰æ˜¯å¤§å¤šæ•°æ·±åº¦å­¦ä¹ è®¡ç®—çš„æ ¸å¿ƒï¼šç»™å®šçŸ©é˜µ A ä¸ Bï¼Œè®¡ç®—å®ƒä»¬çš„ä¹˜ç§¯ C = A Ã— Bã€‚
è¯¥å®ç°ä½¿ç”¨ **FP8**ï¼ˆä½ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼‰ï¼Œä»¥å°‘é‡ç²¾åº¦æ¢å–æ›´é«˜çš„ååé‡å’Œæ›´ä½çš„æ˜¾å­˜å¸¦å®½éœ€æ±‚ã€‚æ­¤å†…æ ¸ä¸º [AMD Developer Challenge 2025](https://www.datamonsters.com/amd-developer-challenge-2025) å¼€å‘ï¼Œå¹¶åœ¨ 2025 å¹´ 6 æœˆè·å¾— ğŸ† **ç‰¹ç­‰å¥–**ï¼Œä»¥è¡¨å½°å…¶åœ¨ AMD ç¡¬ä»¶ä¸Šçš„æ€§èƒ½ä¸åˆ›æ–°è¡¨ç°ã€‚

è¯¥å†…æ ¸ä½¿ç”¨ `e4m3fnuz` æµ®ç‚¹æ ¼å¼è¿›è¡Œé‡åŒ–è®¡ç®—ï¼Œå¹¶é€šè¿‡åˆ†å—ç¼©æ”¾ä¿æŒä½ç²¾åº¦è®¡ç®—çš„å‡†ç¡®æ€§ã€‚`e4m3fnuz` æ˜¯ä¸€ç§ FP8 å˜ä½“ï¼Œæ‹¥æœ‰ 4 ä½æŒ‡æ•°å’Œ 3 ä½å°¾æ•°ï¼Œä¸“ä¸ºç¥ç»ç½‘ç»œä»»åŠ¡è®¾è®¡ã€‚ç”±äº FP8 çš„åŠ¨æ€èŒƒå›´è¾ƒå°ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå—åº”ç”¨ç¼©æ”¾å› å­ï¼ˆa_scale å’Œ b_scaleï¼‰ï¼Œä»¥åœ¨è®¡ç®—å‰åå°†æ•°å€¼è°ƒæ•´åˆ°åˆç†èŒƒå›´ï¼Œä»è€Œå°½å¯èƒ½ä¿ç•™ç²¾åº¦ã€‚

å‡½æ•°æ¥å£å¦‚ä¸‹ï¼š

```
(a, b, a_scale, b_scale, c)
```

å‚æ•°å«ä¹‰ï¼š

* `a`: è¾“å…¥çŸ©é˜µ Aï¼Œå¤§å°ä¸º K Ã— Mï¼Œç±»å‹ e4m3fnuz
* `b`: è¾“å…¥çŸ©é˜µ Bï¼Œå¤§å°ä¸º K Ã— Nï¼Œç±»å‹ e4m3fnuz
* `a_scale`: å¤§å° (K // 128) Ã— Mï¼Œç±»å‹ fp32
* `b_scale`: å¤§å° (K // 128) Ã— (N // 128)ï¼Œç±»å‹ fp32
* `c`: è¾“å‡ºçŸ©é˜µï¼Œå¤§å° M Ã— Nï¼Œç±»å‹ bf16

è¯¥å†…æ ¸é’ˆå¯¹ç‰¹å®šçŸ©é˜µå½¢çŠ¶è¿›è¡Œäº†é¢„ç¼–è¯‘ï¼Œå¹¶å‡è®¾å†…å­˜ä¸ºè½¬ç½®å¸ƒå±€ï¼ˆæ¯”èµ›è¦æ±‚ï¼‰ã€‚è‹¥éœ€æ”¯æŒæ›´å¤šå½¢çŠ¶æˆ–å¸ƒå±€ï¼Œéœ€è¦ä¿®æ”¹å¯åŠ¨ä»£ç ã€‚

ç°åœ¨æˆ‘ä»¬å·²æœ‰ä¸€ä¸ªé«˜æ€§èƒ½ ROCm å†…æ ¸ï¼Œæ¥ä¸‹æ¥è¦åšçš„æ˜¯ï¼š**å¦‚ä½•å°†å®ƒæ•´åˆåˆ° PyTorch ä¸­å¹¶åˆ†äº«ï¼Ÿ**
æ¥ä¸‹æ¥æˆ‘ä»¬å°†åˆ©ç”¨ `kernel-builder` ä¸ `kernels` è¿›è¡Œé¡¹ç›®ç»“æ„åŒ–ã€æ„å»ºä¸å‘å¸ƒã€‚

> [!NOTE]
> æœ¬æ•™ç¨‹æŠ€æœ¯æ€§è¾ƒå¼ºï¼Œä½†ä½ å¯ä»¥ç…§ç€ä¸€æ­¥æ­¥æ“ä½œï¼Œæ— éœ€ç†è§£æ‰€æœ‰ç»†èŠ‚ï¼Œä¹Ÿèƒ½é¡ºåˆ©è¿è¡Œã€‚æƒ³æ·±å…¥å­¦ä¹ æ—¶å¯éšæ—¶å›å¤´é˜…è¯»ã€‚

### æ­¥éª¤ 1ï¼šé¡¹ç›®ç»“æ„

Hugging Face çš„ Kernel Builder æœŸæœ›é¡¹ç›®æ–‡ä»¶æŒ‰ä»¥ä¸‹æ–¹å¼ç»„ç»‡ï¼š

```
gemm/
â”œâ”€â”€ build.toml
â”œâ”€â”€ gemm
â”‚   â””â”€â”€ gemm_kernel.h
â”œâ”€â”€ flake.nix
â””â”€â”€ torch-ext
    â”œâ”€â”€ torch_binding.cpp
    â”œâ”€â”€ torch_binding.h
    â””â”€â”€ gemm
        â””â”€â”€ __init__.py
```

* **build.toml**ï¼šé¡¹ç›®æ„å»ºé…ç½®æ–‡ä»¶ï¼Œå®šä¹‰æ•´ä¸ªç¼–è¯‘è¿‡ç¨‹ã€‚
* **gemm/**ï¼šGPU æºç ç›®å½•ã€‚
* **flake.nix**ï¼šå¯å¤ç°æ„å»ºç¯å¢ƒé…ç½®ã€‚
* **torch-ext/**ï¼šPyTorch æ‰©å±•çš„ Python å°è£…ã€‚

å®é™…é¡¹ç›®å¯èƒ½è¿˜åŒ…å«æµ‹è¯•ã€è„šæœ¬ç­‰é™„åŠ æ–‡ä»¶ï¼Œå¯è‡ªç”±æ·»åŠ ã€‚
åœ¨æœ¬æ–‡ç¤ºä¾‹ä¸­ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```
gemm/
â”œâ”€â”€ build.toml
â”œâ”€â”€ gemm
â”‚   â”œâ”€â”€ gemm_kernel.h
â”‚   â”œâ”€â”€ gemm_kernel_legacy.h
â”‚   â”œâ”€â”€ transpose_kernel.h
â”‚   â””â”€â”€ gemm_launcher.hip
â”œâ”€â”€ include
â”‚   â”œâ”€â”€ clangd_workaround.h
â”‚   â”œâ”€â”€ gpu_libs.h
â”‚   â”œâ”€â”€ gpu_types.h
â”‚   â””â”€â”€ timer.h
â”œâ”€â”€ src/utils
â”‚   â”œâ”€â”€ arithmetic.h
â”‚   â””â”€â”€ timer.hip
â”œâ”€â”€ tests/checker
â”‚   â”œâ”€â”€ checker.cpp
â”‚   â”œâ”€â”€ metrics.h
â”‚   â””â”€â”€ checker.h
â”œâ”€â”€ flake.nix
â””â”€â”€ torch-ext
    â”œâ”€â”€ torch_binding.cpp
    â”œâ”€â”€ torch_binding.h
    â””â”€â”€ gemm
        â””â”€â”€ __init__.py
```

å¦‚æœä½ æŸ¥çœ‹ RadeonFlow Kernels ä¸­ GEMM å†…æ ¸çš„åŸå§‹æ–‡ä»¶ï¼Œä¼šå‘ç°å®ƒä»¬æ˜¯ä»¥ `.cpp` ä¸ºåç¼€çš„ HIP æºæ–‡ä»¶ã€‚
åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¬¬ä¸€æ­¥éœ€è¦æ ¹æ®æ–‡ä»¶å†…å®¹å’Œç”¨é€”ï¼Œå°†è¿™äº›æ‰©å±•åä¿®æ”¹ä¸º `.h` æˆ– `.hip`ï¼š

* ä½¿ç”¨ `.h`ï¼šé€‚ç”¨äºåŒ…å«å†…æ ¸å£°æ˜ã€å†…è”å‡½æ•°æˆ–æ¨¡æ¿ä»£ç çš„å¤´æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶é€šå¸¸ä¼šè¢«å…¶ä»–æ–‡ä»¶å¼•ç”¨ã€‚
* ä½¿ç”¨ `.hip`ï¼šé€‚ç”¨äºåŒ…å«éœ€è¦å•ç‹¬ç¼–è¯‘çš„ HIP/GPU å®ç°ä»£ç çš„æ–‡ä»¶ï¼ˆä¾‹å¦‚å†…æ ¸å¯åŠ¨å™¨ã€å¤æ‚çš„è®¾å¤‡å‡½æ•°ç­‰ï¼‰ã€‚

ä¾‹å¦‚ï¼š`gemm_kernel.h`ã€`gemm_kernel_legacy.h`ã€`transpose_kernel.h` æ˜¯å¤´æ–‡ä»¶ï¼Œ`gemm_launcher.hip` æ˜¯å®ç°æ–‡ä»¶ã€‚
è¿™ç§å‘½åæ–¹å¼æœ‰åŠ©äº kernel-builder æ­£ç¡®è¯†åˆ«å’Œç¼–è¯‘ã€‚

### æ­¥éª¤ 2ï¼šé…ç½®æ–‡ä»¶è®¾ç½®

#### `build.toml` æ„å»ºæ¸…å•

è¿™ä¸ªæ–‡ä»¶è´Ÿè´£ç»Ÿç­¹æ•´ä¸ªæ„å»ºè¿‡ç¨‹ï¼Œå‘Šè¯‰ kernel-builder è¦ç¼–è¯‘å“ªäº›å†…å®¹ä»¥åŠå®ƒä»¬ä¹‹é—´å¦‚ä½•å…³è”ã€‚

```toml
[general]
name = "gemm"
universal = false

[torch]
src = [
  "torch-ext/torch_binding.cpp",
  "torch-ext/torch_binding.h",
]

[kernel.gemm]
backend = "rocm"
rocm-archs = [
    "gfx942",
]

depends = ["torch"]

src = [
  "include/clangd_workaround.h",
  "include/gpu_libs.h",
  "include/gpu_types.h",
  "include/timer.h",
  "gemm/gemm_kernel.h",
  "gemm/gemm_kernel_legacy.h",
  "gemm/gemm_launcher.hip",
  "gemm/transpose_kernel.h",
  "src/utils/arithmetic.h",
  "src/utils/timer.hip",
  "tests/checker/metrics.h",
]

include = ["include"]
```

**general**

è¿™ä¸€éƒ¨åˆ†å®šä¹‰é¡¹ç›®çš„åŸºæœ¬é…ç½®ã€‚

* **name**ï¼ˆå¿…å¡«ï¼‰ï¼šé¡¹ç›®åç§°ï¼Œåº”ä¸å†…æ ¸åä¸€è‡´ï¼Œä¹Ÿä¼šä½œä¸º Python åŒ…åä½¿ç”¨ã€‚
* **universal**ï¼ˆå¯é€‰ï¼‰ï¼šè®¾ä¸º `true` æ—¶è¡¨ç¤ºè¯¥å†…æ ¸æ˜¯é€šç”¨å†…æ ¸ï¼ˆçº¯ Python å®ç°ï¼Œæ— éœ€ç¼–è¯‘ï¼‰ã€‚é€šç”¨å†…æ ¸ä¸ä¼šä½¿ç”¨ä¸‹æ–¹çš„å…¶ä»–é…ç½®éƒ¨åˆ†ã€‚å…¸å‹ç¤ºä¾‹æ˜¯ Triton å†…æ ¸ã€‚é»˜è®¤å€¼ï¼š`false`ã€‚

**torch**

è¿™ä¸€éƒ¨åˆ†æè¿° PyTorch æ‰©å±•çš„é…ç½®ï¼Œç”¨äºå®šä¹‰å°†å†…æ ¸æš´éœ²ç»™ PyTorch çš„ Python ç»‘å®šæ¥å£ã€‚

* **src**ï¼ˆå¿…å¡«ï¼‰ï¼šåˆ—å‡ºç”¨äºæ„å»º PyTorch æ‰©å±•çš„æºæ–‡ä»¶ä¸å¤´æ–‡ä»¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åŒ…å«åˆ›å»º Python æ¥å£çš„ C++ ç»‘å®šæ–‡ä»¶ã€‚

**kernel.gemm**

å®šä¹‰åä¸º â€œgemmâ€ çš„å†…æ ¸ã€‚è‹¥é¡¹ç›®ä¸­åŒ…å«å¤šä¸ªå†…æ ¸ï¼Œå¯åœ¨åŒä¸€ä¸ª `build.toml` æ–‡ä»¶ä¸­æ·»åŠ å¤šä¸ª `[kernel.xxx]` éƒ¨åˆ†ã€‚

* **backend**ï¼ˆå¿…å¡«ï¼‰ï¼šè®¡ç®—åç«¯ç±»å‹ï¼Œè¿™é‡Œä½¿ç”¨ â€œrocmâ€ è¡¨ç¤º AMD GPUã€‚
* **rocm-archs**ï¼ˆROCm å¿…å¡«ï¼‰ï¼šæŒ‡å®šç¼–è¯‘ç›®æ ‡çš„ ROCm æ¶æ„åˆ—è¡¨ï¼Œä¾‹å¦‚ â€œgfx942â€ å¯¹åº” MI300 ç³»åˆ— GPUã€‚
* **depends**ï¼ˆå¿…å¡«ï¼‰ï¼šä¾èµ–é¡¹åˆ—è¡¨ã€‚æ­¤å¤„ä¾èµ– â€œtorchâ€ï¼Œä»¥ä¾¿ä½¿ç”¨ PyTorch å¼ é‡æ“ä½œã€‚
* **include**ï¼ˆå¯é€‰ï¼‰ï¼šç›¸å¯¹é¡¹ç›®æ ¹ç›®å½•çš„å¤´æ–‡ä»¶æœç´¢è·¯å¾„ï¼Œæ–¹ä¾¿ç¼–è¯‘å™¨æŸ¥æ‰¾ä¾èµ–ã€‚

#### `flake.nix` å¯å¤ç°æ€§é…ç½®æ–‡ä»¶

ä¸ºäº†è®©ä»»ä½•äººéƒ½èƒ½åœ¨ä»»æ„æœºå™¨ä¸Šæ„å»ºä½ çš„å†…æ ¸ï¼Œæˆ‘ä»¬ä½¿ç”¨ `flake.nix` æ–‡ä»¶æ¥é”å®š kernel-builder åŠå…¶ä¾èµ–çš„ç¡®åˆ‡ç‰ˆæœ¬ã€‚ï¼ˆå¯ä»¥ç›´æ¥å¤åˆ¶ä¸‹é¢çš„ç¤ºä¾‹å¹¶ä¿®æ”¹æè¿°å³å¯ï¼‰

```nix
{
  description = "Flake for GEMM kernel";

  inputs = {
    kernel-builder.url = "github:huggingface/kernel-builder";
  };

  outputs =
    {
      self,
      kernel-builder,
    }:

    kernel-builder.lib.genFlakeOutputs {
      inherit self;
      path = ./.;
    };
}
```

#### ç¼–å†™å†…æ ¸

æ¥ä¸‹æ¥æ˜¯ GPU ä»£ç ã€‚åœ¨ `gemm/gemm_launcher.hip` æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰ GEMM å†…æ ¸çš„å¯åŠ¨é€»è¾‘ã€‚
æ ¹æ®é…ç½®ï¼Œç¨‹åºä¼šé€‰æ‹©è°ƒç”¨æ–°çš„ä¼˜åŒ–ç‰ˆ `gemm/gemm_kernel`ï¼Œæˆ–åœ¨å¿…è¦æ—¶å›é€€åˆ°æ—§ç‰ˆ `gemm/gemm_kernel_legacy`ã€‚

```C
// ... previous includes and definitions
extern "C" void run(
    void *a, void *b, void *as, void *bs, void *c,
    int m, int n, int k,
    PerfMetrics *metrics, hipStream_t job_stream0
) {
    const __FP8_TYPE *a_ptr = static_cast<const __FP8_TYPE *>(a);
    const __FP8_TYPE *b_ptr = static_cast<const __FP8_TYPE *>(b);
    __BF16_TYPE *c_ptr = static_cast<__BF16_TYPE *>(c);
    const float *as_ptr = static_cast<const float *>(as);
    const float *bs_ptr = static_cast<const float *>(bs);

    KernelTimerScoped timer(timers, 2LL * m * n * k,
        metrics ? &metrics->entries[0].time : nullptr,
        metrics ? &metrics->entries[0].gflops : nullptr, job_stream0);

    // Dispatch GEMM to the fastest available implementation
    switch (pack_shape(m, n, k)) {
        DISPATCH_GEMM(1024, 1536, 7168, 256, 128, 128, 4, 2, 512, 4, 16);
        DISPATCH_GEMM(6144, 7168, 2304, 256, 128, 128, 4, 2, 512, 1, 16);
        default: {
            printf("Error: Unsupported shape M=%d, K=%d, N=%d\n", m, k, n);
            abort();
        }
    }
}
// ...
```

#### æ³¨å†ŒåŸç”Ÿ PyTorch è¿ç®—ç¬¦

è¿™ä¸€æ­¥éå¸¸å…³é”®ã€‚æˆ‘ä»¬ä¸ä»…æ˜¯è®©å‡½æ•°èƒ½åœ¨ Python ä¸­è¢«è°ƒç”¨ï¼Œè€Œæ˜¯è¦æŠŠå®ƒæ³¨å†Œä¸º **åŸç”Ÿ PyTorch è¿ç®—ç¬¦**ï¼Œè®©å®ƒæˆä¸º `torch.ops` ä¸‹çš„ä¸€ç­‰æˆå‘˜ã€‚

`torch-ext/torch_binding.cpp` æ–‡ä»¶è´Ÿè´£è¿™ä¸ªæ³¨å†Œè¿‡ç¨‹ã€‚

```C
#include <torch/all.h>
#include <torch/library.h>
#include <hip/hip_runtime.h>

#include "registration.h"
#include "torch_binding.h"

// Forward declaration of the C function from gemm_launcher.hip
extern "C" {
    struct PerfMetrics;
    void run(void *a, void *b, void *as, void *bs, void *c, int m, int n, int k, PerfMetrics *metrics, hipStream_t job_stream0);
}

void gemm(torch::Tensor &out, torch::Tensor const &a, torch::Tensor const &b, 
          torch::Tensor const &as, torch::Tensor const &bs) {
    
    // Validate tensor properties
    TORCH_CHECK(a.device().is_cuda(), "Input tensor a must be on GPU device");
    TORCH_CHECK(b.device().is_cuda(), "Input tensor b must be on GPU device");
    TORCH_CHECK(as.device().is_cuda(), "Scale tensor as must be on GPU device");
    TORCH_CHECK(bs.device().is_cuda(), "Scale tensor bs must be on GPU device");
    TORCH_CHECK(out.device().is_cuda(), "Output tensor out must be on GPU device");
    
    TORCH_CHECK(a.is_contiguous(), "Input tensor a must be contiguous");
    TORCH_CHECK(b.is_contiguous(), "Input tensor b must be contiguous");
    TORCH_CHECK(as.is_contiguous(), "Scale tensor as must be contiguous");
    TORCH_CHECK(bs.is_contiguous(), "Scale tensor bs must be contiguous");
    TORCH_CHECK(out.is_contiguous(), "Output tensor out must be contiguous");
    
    // Get matrix dimensions from tensor shapes
    // Assuming a is [M, K], b is [K, N], out is [M, N]
    int M = a.size(0);
    int K = a.size(1);
    int N = b.size(1);
    
    TORCH_CHECK(b.size(0) == K, "Matrix dimensions mismatch: a.size(1) != b.size(0)");
    TORCH_CHECK(out.size(0) == M, "Output tensor dimension mismatch: out.size(0) != M");
    TORCH_CHECK(out.size(1) == N, "Output tensor dimension mismatch: out.size(1) != N");
    
    // Use default HIP stream (stream 0)
    const hipStream_t stream = 0;
    
    // Call the C function
    run(a.data_ptr(), b.data_ptr(), as.data_ptr(), bs.data_ptr(), out.data_ptr(),
        M, N, K, nullptr, stream);
}

TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {
  ops.def("gemm(Tensor! out, Tensor a, Tensor b, Tensor a_scale, Tensor b_scale) -> ()");
  ops.impl("gemm", torch::kCUDA, &gemm);
}

REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
```
`torch_binding.h` æ–‡ä»¶ç”¨äºå£°æ˜å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº `gemm` å†…æ ¸ï¼Œå…¶åœ¨ `torch_binding.h` ä¸­çš„å‡½æ•°å£°æ˜å¦‚ä¸‹ï¼š

```h
#pragma once

#include <torch/torch.h>

void gemm(torch::Tensor &out, torch::Tensor const &a, torch::Tensor const &b, 
          torch::Tensor const &as, torch::Tensor const &bs);
```

#### åˆ›å»º `__init__.py` å°è£…

åœ¨ `torch-ext/gemm/` ç›®å½•ä¸‹ï¼Œéœ€è¦ä¸€ä¸ª `__init__.py` æ–‡ä»¶ï¼Œä½¿å…¶æˆä¸º Python åŒ…ï¼Œå¹¶ä»¥æ›´æ˜“ç”¨çš„æ–¹å¼æš´éœ²è‡ªå®šä¹‰è¿ç®—ç¬¦ã€‚

```python
from typing import Optional
import torch
from ._ops import ops

def gemm(a: torch.Tensor, b: torch.Tensor, as_: torch.Tensor, bs: torch.Tensor, 
         out: Optional[torch.Tensor] = None) -> torch.Tensor:
         
    if out is None:
        # Create output tensor with appropriate shape and dtype
        M, K = a.shape
        K_b, N = b.shape
        assert K == K_b, f"Matrix dimension mismatch: A has {K} cols, B has {K_b} rows"
        
        # Output should be BF16 type on the same device as inputs
        out = torch.empty((M, N), dtype=torch.bfloat16, device=a.device)
    
    ops.gemm(out, a, b, as_, bs)
    return out
```

### æ­¥éª¤ 3ï¼šæ„å»ºå†…æ ¸

`kernel-builder` ä½¿ç”¨ **Nix** æ¥æ„å»ºå†…æ ¸ã€‚åªè¦ä½ çš„ç³»ç»Ÿä¸­å®‰è£…äº† Nixï¼Œå°±å¯ä»¥ç›´æ¥ç¼–è¯‘æˆ–è¿è¡Œè¿™äº›å†…æ ¸ã€‚æ¨èçš„å®‰è£…æ–¹å¼å¦‚ä¸‹ï¼š

* **Linux**ï¼šä½¿ç”¨ [å®˜æ–¹ Nix å®‰è£…å™¨](https://nixos.org/download/)ã€‚
* **macOS**ï¼šä½¿ç”¨ [Determinate Nix å®‰è£…å™¨](https://docs.determinate.systems/determinate-nix/)ã€‚å¦å¤–ï¼Œç›®å‰æ„å»ºå†…æ ¸è¿˜éœ€è¦ Xcode 16.xã€‚

#### å¼€å§‹ä½¿ç”¨ Nix

é¦–å…ˆï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
nix flake update
```

è¯¥å‘½ä»¤ä¼šç”Ÿæˆä¸€ä¸ª `flake.lock` æ–‡ä»¶ï¼Œç”¨äºé”å®š kernel-builder åŠå…¶æ‰€æœ‰ä¾èµ–çš„ç¡®åˆ‡ç‰ˆæœ¬ã€‚
è¯·å°† `flake.nix` å’Œ `flake.lock` ä¸€å¹¶æäº¤åˆ°ä»“åº“ä¸­ï¼Œä»¥ç¡®ä¿æ„å»ºç»“æœåœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½å¯å¤ç°ã€‚

ç”±äº kernel-builder ä¾èµ–è®¸å¤šè½¯ä»¶åŒ…ï¼ˆä¾‹å¦‚ä¸åŒç‰ˆæœ¬çš„ PyTorchï¼‰ï¼Œå»ºè®®å¯ç”¨ Hugging Face çš„ç¼“å­˜ä»¥é¿å…é‡å¤æ„å»ºï¼ŒèŠ‚çœå¤§é‡æ—¶é—´ï¼š

```bash
# Install cachix and configure the cache
cachix use huggingface
```

æˆ–è€…å¦‚æœä½ ä¸æƒ³æ°¸ä¹…å®‰è£… cachixï¼Œå¯ä»¥ä»…ä¸´æ—¶å¯ç”¨ä¸€æ¬¡ï¼š

```bash
# Use cachix without installing it
nix run nixpkgs#cachix -- use huggingface

#### ä½¿ç”¨ Nix æ„å»ºå†…æ ¸

å¦‚æœé¡¹ç›®ä¸­åŒ…å« `flake.nix` æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ„å»ºï¼š

```bash
cd Build_RadeonFlow_Kernels/gemm
nix build . -L
```

ç¼–è¯‘å®Œæˆåï¼Œç”Ÿæˆçš„å†…æ ¸æ–‡ä»¶ä¼šä¿å­˜åœ¨æœ¬åœ°çš„ `build/` ç›®å½•ä¸­ã€‚


#### æœ¬åœ°å¼€å‘ç¯å¢ƒï¼ˆDevelopment Shellï¼‰

`kernel-builder` æä¾›äº†é€‚ç”¨äºå¼€å‘çš„ Shell ç¯å¢ƒã€‚åœ¨è¿™ç§ç¯å¢ƒä¸­ï¼Œæ‰€æœ‰ä¾èµ–é¡¹éƒ½ä¼šè¢«è‡ªåŠ¨é…ç½®å¥½ï¼ŒåŒæ—¶æä¾› `build2cmake` å·¥å…·ç”¨äºç”Ÿæˆ CMake é¡¹ç›®æ–‡ä»¶ï¼š

```bash
$ nix develop
$ build2cmake generate-torch build.toml
$ cmake -B build-ext
$ cmake --build build-ext
```

å¦‚æœä½ æƒ³å°†å†…æ ¸ä½œä¸º Python åŒ…è¿›è¡Œæµ‹è¯•ï¼Œä¹Ÿå¯ä»¥ç›´æ¥åœ¨è¿™ä¸ªç¯å¢ƒä¸­å®Œæˆã€‚`nix develop` ä¼šè‡ªåŠ¨åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºå¹¶æ¿€æ´»ä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒ `.venv`ï¼š

```bash
$ nix develop
$ build2cmake generate-torch build.toml
$ pip install --no-build-isolation -e .
```

å¼€å‘ç¯å¢ƒå¯é’ˆå¯¹ä¸åŒçš„æ„å»ºé…ç½®è¿›è¡Œåˆ‡æ¢ã€‚
ä¾‹å¦‚ï¼Œè‹¥ä½ æƒ³åœ¨ **Torch 2.7 + ROCm 6.3** ç¯å¢ƒä¸­è¿›è¡Œå¼€å‘ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›å…¥å¯¹åº”çš„ Shellï¼š

```bash
$ rm -rf .venv  # Remove existing venv if any
$ nix develop .#devShells.torch27-cxx11-rocm63-x86_64-linux
```

è¿™æ ·ï¼Œä½ å°±èƒ½åœ¨å®Œæ•´çš„å¯å¤ç°ç¯å¢ƒä¸­æ„å»ºã€è°ƒè¯•å¹¶æµ‹è¯•ä½ çš„ ROCm å†…æ ¸ã€‚

### æ­¥éª¤ 4ï¼šä¸Šä¼ å†…æ ¸åˆ° Hugging Face Hub

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†å†…æ ¸çš„æ„å»ºï¼Œæ¥ä¸‹æ¥å¯ä»¥è¿›è¡Œæµ‹è¯•å¹¶å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hubã€‚

#### ä¸ºæ‰€æœ‰ PyTorch å’Œ ROCm ç‰ˆæœ¬æ„å»ºå†…æ ¸

åœ¨åˆ†äº«ä¹‹å‰ï¼Œå…ˆæ¸…ç†æ‰æ„å»ºè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶å’Œå¼€å‘äº§ç‰©ï¼Œä»¥é¿å…ä¸Šä¼ ä¸å¿…è¦çš„æ–‡ä»¶ï¼š

```bash
build2cmake clean build.toml 
```

ä¸ºäº†åŒæ—¶æ„å»ºæ‰€æœ‰æ”¯æŒçš„ PyTorch å’Œ ROCm ç‰ˆæœ¬ï¼Œå¯ä»¥ä½¿ç”¨ kernel-builder å·¥å…·è‡ªåŠ¨å®Œæˆè¿™ä¸€è¿‡ç¨‹ï¼š

```bash
# Outside of the dev shell, run the following command
# if you are inside of the sandbox you can leave with `exit`
nix build . -L
``` 

> **æ³¨æ„ï¼š**
> è¿™ä¸ªè¿‡ç¨‹å¯èƒ½ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ï¼Œå› ä¸ºå®ƒä¼šä¸ºæ‰€æœ‰æ”¯æŒçš„ PyTorch ä¸ ROCm ç‰ˆæœ¬è¿›è¡Œç¼–è¯‘ã€‚
> æ„å»ºç»“æœä¼šè¾“å‡ºåˆ° `result` ç›®å½•ä¸­ã€‚

æœ€åï¼Œå°†æ„å»ºç»“æœç§»åŠ¨åˆ°é¡¹ç›®ä¸­é¢„æœŸçš„ `build/` ç›®å½•ä¸­ï¼ˆ`kernels` åº“ä¼šä»è¿™é‡Œè¯»å–ç¼–è¯‘å¥½çš„æ–‡ä»¶ï¼‰ï¼š

```bash
mkdir -p build
rsync -av --delete --chmod=Du+w,Fu+w result/ build/
```

#### æ¨é€åˆ° Hugging Face Hub

å°†æ„å»ºäº§ç‰©æ¨é€åˆ° Hugging Face Hub åï¼Œå…¶ä»–å¼€å‘è€…å°±èƒ½ç›´æ¥ä¸‹è½½å¹¶ä½¿ç”¨ä½ çš„å†…æ ¸ã€‚

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ä»“åº“ï¼š

```bash
hf repo create gemm
```

> è¯·ç¡®ä¿å·²ä½¿ç”¨ `huggingface-cli login` ç™»å½• Hugging Face è´¦æˆ·ã€‚

ç„¶ååœ¨é¡¹ç›®ç›®å½•ä¸­ï¼Œå°†æœ¬åœ°é¡¹ç›®ä¸åˆšåˆ›å»ºçš„ä»“åº“è¿æ¥å¹¶æ¨é€ä»£ç ï¼š

```bash
# Initialize git and connect to the Hugging Face Hub
git init
git remote add origin https://huggingface.co/<your-username>/gemm

# Pull the changes (just the default .gitattributes file)
git pull origin main
git xet install
git checkout -b main

# Update to use Xet for the binary files
git xet track "*.so"

# Add and commit your changes (being careful to only include the necessary files
# since our build2cmake command generated a lot of dev-specific files)
git add \
  build/ gemm/ include/ src/utils tests/checker \
  torch-ext/torch_binding.cpp torch-ext/torch_binding.h torch-ext/gemm \
  flake.nix flake.lock build.toml

git commit -m "feat: Created a compliant gemm kernel"
git push -u origin main
```ush -u origin main
```

ğŸ‰ **å®Œæˆï¼** ä½ çš„å†…æ ¸ç°å·²æˆåŠŸä¸Šä¼ åˆ° Hugging Face Hubã€‚å…¶ä»–å¼€å‘è€…å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒï¼Œå¹¶ä¸”å®ƒå·²å®Œå…¨ç¬¦åˆ `kernels` åº“çš„è§„èŒƒã€‚

### æ­¥éª¤ 5ï¼šè®©æˆ‘ä»¬æ¥ä½¿ç”¨å®ƒå§ :)

åœ¨ **kernels** åº“ä¸­ï¼Œä½ å¹¶ä¸éœ€è¦åƒä¼ ç»Ÿæ–¹å¼é‚£æ ·â€œå®‰è£…â€å†…æ ¸ï¼Œè€Œæ˜¯å¯ä»¥ç›´æ¥ä» Hugging Face Hub åŠ è½½å®ƒã€‚åŠ è½½åï¼Œå†…æ ¸ä¼šè‡ªåŠ¨æ³¨å†Œä¸ºæ–°çš„ PyTorch è¿ç®—ç¬¦ã€‚

```python
import torch
from kernels import get_kernel

# Load the kernel from the Hub
gemm = get_kernel("kernels-community/gemm")

# Matrix dimensions (must be supported - see gemm_launcher.cpp)
M, N, K = 1024, 1536, 7168
QUANT_SIZE = 128

# Setup device
device = torch.device("cuda")

# Create inputs - kernel expects A:(K,M), B:(K,N)
A_fp32 = torch.randn(M, K, device=device)
B_fp32 = torch.randn(K, N, device=device)

# Convert to FP8
A_fp8 = A_fp32.to(torch.float8_e4m3fnuz)
B_fp8 = B_fp32.to(torch.float8_e4m3fnuz)

# Create scale factors (uniform scaling)
A_scale = torch.ones(K // QUANT_SIZE, M, device=device, dtype=torch.float32)
B_scale = torch.ones(K // QUANT_SIZE, N // QUANT_SIZE, device=device, dtype=torch.float32)

C = torch.zeros(M, N, device=device, dtype=torch.bfloat16)

# Use the kernel
result = gemm.gemm(A_fp8, B_fp8, A_scale, B_scale, C)
```

å°±æ˜¯è¿™æ ·ï¼ğŸ‰
ä½ çš„ ROCm å†…æ ¸ç°åœ¨å·²ç»å¯ä»¥ç›´æ¥ä» Hugging Face Hub ä½¿ç”¨å•¦ã€‚

## æ€»ç»“

å€ŸåŠ© Hugging Face æä¾›çš„å·¥å…·ï¼Œæ„å»ºå’Œåˆ†äº« ROCm å†…æ ¸å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€‚
é€šè¿‡ Nix å®ç°çš„å¯å¤ç°æ„å»ºæµç¨‹ä¸ PyTorch çš„æ— ç¼é›†æˆï¼Œå¼€å‘è€…å¯ä»¥æŠŠæ›´å¤šç²¾åŠ›æŠ•å…¥åˆ°æ€§èƒ½ä¼˜åŒ–ä¸Šï¼Œè€Œä¸å†è¢«ç¯å¢ƒé…ç½®å›°æ‰°ã€‚

ä¸€æ—¦æ„å»ºå®Œæˆï¼Œåªéœ€ä¸Šä¼ åˆ° Hugging Face Hubï¼Œç¤¾åŒºä¸­çš„å…¶ä»–äººå°±èƒ½ç›´æ¥ä½¿ç”¨ä½ çš„è‡ªå®šä¹‰å†…æ ¸â€”â€”å‡ è¡Œä»£ç å³å¯å®Œæˆé›†æˆï¼Œè½»æ¾å…±äº«é«˜æ€§èƒ½æˆæœã€‚ğŸš€

## ç›¸å…³åº“ä¸èµ„æº

* [kernel-builder](https://github.com/huggingface/kernel-builder) â€”â€” ç”¨äºæ„å»ºä¸ç¼–è¯‘è‡ªå®šä¹‰å†…æ ¸çš„å·¥å…·
* [kernels](https://github.com/huggingface/kernels) â€”â€” ç”¨äºä» Hub ç®¡ç†ä¸åŠ è½½å†…æ ¸çš„åº“
* [Kernels Community Hub](https://huggingface.co/kernels-community) â€”â€” å‘ç°ä¸åˆ†äº«ç¤¾åŒºè‡ªå®šä¹‰å†…æ ¸çš„å¹³å°

