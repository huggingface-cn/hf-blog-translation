---
title: "ä½¿ç”¨å¼€æ”¾æ¨¡å‹å¢å¼ºä½ çš„ OCR æµç¨‹"
thumbnail: /blog/assets/ocr-open-models/thumbnail.png
authors:
- user: merve
- user: ariG23498
- user: davanstrien
- user: hynky
- user: andito
- user: reach-vb
- user: pcuenq
translators:
- user: chenglu
---

# ç”¨å¼€æºæ¨¡å‹å¼ºåŒ–ä½ çš„ OCR å·¥ä½œæµ

> [!æç¤º]
> æˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸­æ–°å¢äº† [Chandra](https://huggingface.co/datalab-to/chandra) å’Œ [OlmOCR-2](https://huggingface.co/allenai/olmOCR-2-7B-1025)ï¼Œå¹¶é™„ä¸Šäº†å®ƒä»¬åœ¨ OlmOCR åŸºå‡†ä¸Šçš„å¾—åˆ† ğŸ«¡

**æ‘˜è¦ï¼š**
å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰çš„å´›èµ·ï¼Œæ­£åœ¨å½»åº•æ”¹å˜æ–‡æ¡£æ™ºèƒ½ï¼ˆDocument AIï¼‰çš„æ ¼å±€ã€‚æ¯ç§æ¨¡å‹éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå› æ­¤é€‰æ‹©åˆé€‚çš„æ¨¡å‹å˜å¾—æ£˜æ‰‹ã€‚ç›¸æ¯”é—­æºæ¨¡å‹ï¼Œå¼€æºæƒé‡çš„æ¨¡å‹åœ¨æˆæœ¬æ•ˆç‡å’Œéšç§ä¿æŠ¤ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚ä¸ºäº†å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ï¼Œæˆ‘ä»¬æ•´ç†äº†è¿™ä»½æŒ‡å—ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œä½ å°†äº†è§£åˆ°ï¼š

* å½“å‰ OCR æ¨¡å‹çš„æ•´ä½“æ ¼å±€åŠå…¶èƒ½åŠ›
* ä½•æ—¶éœ€è¦å¾®è°ƒæ¨¡å‹ï¼Œä½•æ—¶å¯ç›´æ¥ä½¿ç”¨
* ä¸ºä½ çš„åœºæ™¯é€‰æ‹©åˆé€‚æ¨¡å‹æ—¶åº”è€ƒè™‘çš„å…³é”®å› ç´ 
* å¦‚ä½•è¶…è¶Šä¼ ç»Ÿ OCRï¼Œæ¢ç´¢å¤šæ¨¡æ€æ£€ç´¢ä¸æ–‡æ¡£é—®ç­”

è¯»å®Œä¹‹åï¼Œä½ å°†çŸ¥é“å¦‚ä½•é€‰æ‹©åˆé€‚çš„ OCR æ¨¡å‹ã€å¼€å§‹æ„å»ºåº”ç”¨ï¼Œå¹¶å¯¹æ–‡æ¡£ AI æœ‰æ›´æ·±å…¥çš„ç†è§£ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

---

## ç›®å½•

* [ä½¿ç”¨å¼€æºæ¨¡å‹æå‡ä½ çš„ OCR æµæ°´çº¿èƒ½åŠ›](#ä½¿ç”¨å¼€æºæ¨¡å‹æå‡ä½ çš„-ocr-æµæ°´çº¿èƒ½åŠ›)

  * [ç°ä»£ OCR ç®€ä»‹](#ç°ä»£-ocr-ç®€ä»‹)

    * [æ¨¡å‹èƒ½åŠ›](#æ¨¡å‹èƒ½åŠ›)

      * [æ–‡å­—è¯†åˆ«](#æ–‡å­—è¯†åˆ«)
      * [å¤„ç†æ–‡æ¡£ä¸­çš„å¤æ‚ç»„ä»¶](#å¤„ç†æ–‡æ¡£ä¸­çš„å¤æ‚ç»„ä»¶)
      * [è¾“å‡ºæ ¼å¼](#è¾“å‡ºæ ¼å¼)
      * [æœ¬åœ°æ€§æ„è¯†ï¼ˆLocality Awarenessï¼‰](#æœ¬åœ°æ€§æ„è¯†-locality-awareness)
      * [æ¨¡å‹æç¤ºè¯èƒ½åŠ›](#æ¨¡å‹æç¤ºè¯èƒ½åŠ›)
  * [å‰æ²¿å¼€æº OCR æ¨¡å‹](#å‰æ²¿å¼€æº-ocr-æ¨¡å‹)

    * [æ¨¡å‹å¯¹æ¯”](#æ¨¡å‹å¯¹æ¯”)
    * [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)

      * [è¯„æµ‹åŸºå‡†](#è¯„æµ‹åŸºå‡†)
      * [æ€§ä»·æ¯”](#æ€§ä»·æ¯”)
      * [å¼€æº OCR æ•°æ®é›†](#å¼€æº-ocr-æ•°æ®é›†)
  * [æ¨¡å‹è¿è¡Œå·¥å…·](#æ¨¡å‹è¿è¡Œå·¥å…·)

    * [æœ¬åœ°è¿è¡Œ](#æœ¬åœ°è¿è¡Œ)
    * [è¿œç¨‹è¿è¡Œ](#è¿œç¨‹è¿è¡Œ)
  * [è¶…è¶Š OCR çš„èƒ½åŠ›](#è¶…è¶Š-ocr-çš„èƒ½åŠ›)

    * [è§†è§‰æ–‡æ¡£æ£€ç´¢å™¨](#è§†è§‰æ–‡æ¡£æ£€ç´¢å™¨)
    * [åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æ¡£é—®ç­”](#åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æ¡£é—®ç­”)
  * [æ€»ç»“](#æ€»ç»“)

---

## ç°ä»£ OCR ç®€ä»‹

å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOptical Character Recognitionï¼Œç®€ç§° OCRï¼‰æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€æ—©ã€ä¹Ÿæ˜¯æŒç»­æ—¶é—´æœ€é•¿çš„ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚AI çš„è®¸å¤šæ—©æœŸå®é™…åº”ç”¨éƒ½é›†ä¸­åœ¨â€œå°†å°åˆ·æ–‡å­—è½¬åŒ–ä¸ºå¯ç¼–è¾‘çš„æ•°å­—æ–‡æœ¬â€ä¸Šã€‚

éšç€[è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰](https://huggingface.co/blog/vlms)çš„å…´èµ·ï¼ŒOCR çš„èƒ½åŠ›è¿æ¥äº†é£è·ƒå¼æå‡ã€‚å¦‚ä»Šï¼Œè®¸å¤š OCR æ¨¡å‹éƒ½æ˜¯åœ¨ç°æœ‰ VLM çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒå¾—åˆ°çš„ã€‚ä½†ç°ä»£æ¨¡å‹çš„èƒ½åŠ›å·²è¿œè¶…ä¼ ç»Ÿ OCR â€”â€” ä½ ä¸ä»…å¯ä»¥è¯†åˆ«æ–‡å­—ï¼Œè¿˜èƒ½åŸºäºå†…å®¹æ£€ç´¢æ–‡æ¡£ï¼Œç”šè‡³ç›´æ¥è¿›è¡Œé—®ç­”ã€‚

å¾—ç›Šäºæ›´å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤„ç†ä½è´¨é‡æ‰«æä»¶ã€ç†è§£å¤æ‚å…ƒç´ ï¼ˆå¦‚è¡¨æ ¼ã€å›¾è¡¨ã€å›¾ç‰‡ç­‰ï¼‰ï¼Œå¹¶å°†æ–‡æœ¬ä¸è§†è§‰å†…å®¹èåˆï¼Œä»¥å›ç­”è·¨æ–‡æ¡£çš„å¼€æ”¾å¼é—®é¢˜ã€‚

---

### æ¨¡å‹èƒ½åŠ›

#### æ–‡æœ¬è¯†åˆ«

æœ€æ–°çš„æ¨¡å‹èƒ½å¤Ÿå°†å›¾åƒä¸­çš„æ–‡å­—è½¬å½•ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚è¾“å…¥å†…å®¹å¯èƒ½åŒ…æ‹¬ï¼š

* æ‰‹å†™æ–‡å­—
* å„ç±»æ–‡å­—ä½“ç³»ï¼ˆå¦‚æ‹‰ä¸æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ã€æ—¥æ–‡ç­‰ï¼‰
* æ•°å­¦å…¬å¼
* åŒ–å­¦æ–¹ç¨‹å¼
* å›¾ç‰‡ã€ç‰ˆé¢æˆ–é¡µç æ ‡ç­¾

OCR æ¨¡å‹ä¼šå°†è¿™äº›å†…å®¹è½¬æ¢ä¸ºæœºå™¨å¯è¯»çš„æ–‡æœ¬ï¼Œè¾“å‡ºæ ¼å¼å¤šç§å¤šæ ·ï¼Œæ¯”å¦‚ **HTMLã€Markdown** ç­‰ã€‚

---

#### å¤„ç†æ–‡æ¡£ä¸­çš„å¤æ‚ç»„ä»¶

é™¤äº†æ–‡å­—ï¼ŒæŸäº›æ¨¡å‹è¿˜èƒ½è¯†åˆ«ï¼š

* å›¾ç‰‡
* å›¾è¡¨
* è¡¨æ ¼

éƒ¨åˆ†æ¨¡å‹èƒ½è¯†åˆ«æ–‡æ¡£ä¸­å›¾ç‰‡çš„ç²¾ç¡®ä½ç½®ï¼Œæå–å…¶åæ ‡ï¼Œå¹¶åœ¨è¾“å‡ºä¸­å°†å›¾ç‰‡åµŒå…¥å¯¹åº”ä½ç½®ã€‚
å¦ä¸€äº›æ¨¡å‹è¿˜èƒ½ä¸ºå›¾ç‰‡ç”Ÿæˆè¯´æ˜æ–‡å­—ï¼ˆcaptionï¼‰ï¼Œå¹¶åœ¨é€‚å½“ä½ç½®æ’å…¥ã€‚è¿™å¯¹äºåç»­å°†æœºå™¨å¯è¯»è¾“å‡ºä¼ å…¥ LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å°¤ä¸ºæœ‰ç”¨ã€‚

ä¾‹å¦‚ï¼Œ[OlmOCRï¼ˆAllenAI å‡ºå“ï¼‰](https://huggingface.co/allenai/olmOCR-7B-0825) å’Œ [PaddleOCR-VLï¼ˆPaddlePaddle å‡ºå“ï¼‰](https://huggingface.co/PaddlePaddle/PaddleOCR-VL) å°±æ˜¯ä»£è¡¨ã€‚

ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„è¾“å‡ºæ ¼å¼ï¼Œä¾‹å¦‚ **DocTags**ã€**HTML**ã€**Markdown**ï¼ˆåæ–‡ *è¾“å‡ºæ ¼å¼* ä¸€èŠ‚æœ‰è¯¦ç»†è¯´æ˜ï¼‰ã€‚
æ¨¡å‹å¤„ç†è¡¨æ ¼ä¸å›¾è¡¨çš„æ–¹å¼é€šå¸¸å–å†³äºæ‰€é‡‡ç”¨çš„è¾“å‡ºæ ¼å¼ï¼š

* æœ‰äº›æ¨¡å‹å°†å›¾è¡¨å½“ä½œå›¾ç‰‡ç›´æ¥ä¿ç•™ï¼›
* æœ‰äº›æ¨¡å‹åˆ™ä¼šå°†å…¶è½¬æ¢ä¸ºå¯è§£æçš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¦‚ Markdown è¡¨æ ¼æˆ– JSONã€‚
  ä¾‹å¦‚ï¼Œä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªæŸ±çŠ¶å›¾å¦‚ä½•è¢«è½¬æ¢æˆæœºå™¨å¯è¯»çš„å½¢å¼ï¼š

![Chart Rendering](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/chart-rendering.png)

åŒæ ·åœ°ï¼Œè¡¨æ ¼ä¸­çš„å•å…ƒæ ¼ä¹Ÿä¼šè¢«è§£æä¸ºæœºå™¨å¯è¯»æ ¼å¼ï¼Œå¹¶ä¿ç•™åˆ—åä¸æ ‡é¢˜çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼š

![Table Rendering](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/table-rendering.png)

---

#### è¾“å‡ºæ ¼å¼

ä¸åŒ OCR æ¨¡å‹é‡‡ç”¨çš„è¾“å‡ºæ ¼å¼ä¸åŒï¼Œä»¥ä¸‹æ˜¯å‡ ç§ä¸»æµæ ¼å¼çš„ç®€ä»‹ï¼š

* **DocTagï¼š** ä¸€ç§ç±»ä¼¼ XML çš„æ–‡æ¡£æ ‡è®°æ ¼å¼ï¼Œå¯è¡¨è¾¾ä½ç½®ä¿¡æ¯ã€æ–‡æœ¬æ ·å¼ã€ç»„ä»¶å±‚çº§ç­‰ã€‚ä¸‹å›¾å±•ç¤ºäº†ä¸€ç¯‡è®ºæ–‡å¦‚ä½•è¢«è§£æä¸º DocTagsã€‚è¯¥æ ¼å¼ç”±å¼€æºçš„ Docling æ¨¡å‹ä½¿ç”¨ã€‚

  ![DocTags](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/doctags_v2.png)

* **HTMLï¼š** æ˜¯æœ€å¸¸è§çš„æ–‡æ¡£è§£ææ ¼å¼ä¹‹ä¸€ï¼Œèƒ½è¾ƒå¥½åœ°è¡¨è¾¾ç»“æ„ä¸å±‚çº§ä¿¡æ¯ã€‚

* **Markdownï¼š** äººç±»å¯è¯»æ€§æœ€å¼ºï¼Œæ ¼å¼ç®€æ´ï¼Œä½†è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼ˆå¦‚æ— æ³•å‡†ç¡®è¡¨ç¤ºå¤šåˆ—è¡¨æ ¼ï¼‰ã€‚

* **JSONï¼š** é€šå¸¸ç”¨äºè¡¨ç¤ºè¡¨æ ¼æˆ–å›¾è¡¨ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œè€Œéå®Œæ•´æ–‡æ¡£ã€‚

é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œå–å†³äºä½ å¯¹è¾“å‡ºç»“æœçš„ç”¨é€”ï¼š

| ç›®æ ‡åœºæ™¯                | æ¨èæ ¼å¼                            |
| ------------------- | ------------------------------- |
| **æ•°å­—åŒ–é‡å»º**ï¼ˆé‡ç°åŸå§‹æ–‡æ¡£ç‰ˆå¼ï¼‰ | ä½¿ç”¨ä¿ç•™å¸ƒå±€çš„æ ¼å¼ï¼Œå¦‚ DocTags æˆ– HTML      |
| **LLM è¾“å…¥æˆ–é—®ç­”åœºæ™¯**     | ä½¿ç”¨è¾“å‡º Markdown å’Œå›¾åƒè¯´æ˜çš„æ¨¡å‹ï¼ˆæ›´æ¥è¿‘è‡ªç„¶è¯­è¨€ï¼‰ |
| **ç¨‹åºåŒ–å¤„ç†**ï¼ˆå¦‚æ•°æ®åˆ†æï¼‰    | é€‰æ‹©èƒ½è¾“å‡ºç»“æ„åŒ– JSON çš„æ¨¡å‹               |

---

#### OCR çš„ä½ç½®æ„ŸçŸ¥

æ–‡æ¡£å¸¸å¸¸ç»“æ„å¤æ‚ï¼Œæ¯”å¦‚å¤šæ æ–‡æœ¬ã€æµ®åŠ¨å›¾ç‰‡ã€è„šæ³¨ç­‰ã€‚æ—©æœŸçš„ OCR æ¨¡å‹é€šå¸¸å…ˆè¯†åˆ«æ–‡å­—ï¼Œå†é€šè¿‡åå¤„ç†æ‰‹åŠ¨æ¨æ–­é¡µé¢å¸ƒå±€ï¼Œä»¥æ¢å¤é˜…è¯»é¡ºåºâ€”â€”è¿™ç§æ–¹å¼æ—¢è„†å¼±åˆæ˜“é”™ã€‚

ç°ä»£ OCR æ¨¡å‹åˆ™ä¼šåœ¨è¾“å‡ºä¸­ç›´æ¥åŒ…å«ç‰ˆé¢å¸ƒå±€ä¿¡æ¯ï¼ˆç§°ä¸º **â€œé”šç‚¹â€æˆ– â€œgroundingâ€**ï¼‰ï¼Œå¦‚æ–‡å­—çš„è¾¹ç•Œæ¡†ï¼ˆbounding boxï¼‰ã€‚
è¿™ç§â€œé”šå®šâ€æœºåˆ¶èƒ½æœ‰æ•ˆä¿æŒé˜…è¯»é¡ºåºä¸è¯­ä¹‰è¿è´¯æ€§ï¼ŒåŒæ—¶å‡å°‘â€œå¹»è§‰å¼è¯†åˆ«â€ï¼ˆå³é”™è¯¯ç”Ÿæˆå†…å®¹ï¼‰ã€‚

---

#### æ¨¡å‹æç¤º

OCR æ¨¡å‹é€šå¸¸æ¥æ”¶å›¾åƒè¾“å…¥ï¼Œå¹¶å¯é€‰åœ°æ¥å—æ–‡å­—æç¤ºï¼ˆpromptï¼‰ï¼Œè¿™å–å†³äºæ¨¡å‹çš„æ¶æ„ä¸é¢„è®­ç»ƒæ–¹å¼ã€‚

éƒ¨åˆ†æ¨¡å‹æ”¯æŒ**åŸºäºæç¤ºçš„ä»»åŠ¡åˆ‡æ¢**ï¼Œä¾‹å¦‚ [granite-docling](https://huggingface.co/ibm-granite/granite-docling-258M) å¯ä»¥é€šè¿‡ä¸åŒæç¤ºè¯æ‰§è¡Œä¸åŒä»»åŠ¡ï¼š

* è¾“å…¥ â€œConvert this page to Doclingâ€ â†’ å°†æ•´é¡µè½¬æ¢ä¸º DocTagsï¼›
* è¾“å…¥ â€œConvert this formula to LaTeXâ€ â†’ å°†é¡µé¢ä¸­çš„å…¬å¼è½¬æ¢ä¸º LaTeXã€‚

è€Œå¦ä¸€äº›æ¨¡å‹åˆ™åªèƒ½å¤„ç†æ•´é¡µå†…å®¹ï¼Œä»»åŠ¡ç”±ç³»ç»Ÿæç¤ºå›ºå®šå®šä¹‰ã€‚
ä¾‹å¦‚ï¼Œ[OlmOCRï¼ˆAllenAIï¼‰](https://huggingface.co/collections/allenai/olmocr-67af8630b0062a25bf1b54a1) ä½¿ç”¨ä¸€ä¸ªé•¿ç³»ç»Ÿæç¤ºè¯è¿›è¡Œæ¨ç†ã€‚OlmOCR æœ¬è´¨ä¸Šæ˜¯åŸºäº Qwen2.5VL å¾®è°ƒçš„ OCR æ¨¡å‹ï¼Œè™½ç„¶å®ƒä¹Ÿèƒ½å¤„ç†å…¶ä»–ä»»åŠ¡ï¼Œä½†åœ¨ OCR åœºæ™¯ä¹‹å¤–æ€§èƒ½ä¼šæ˜æ˜¾ä¸‹é™ã€‚

---

## å‰æ²¿å¼€æº OCR æ¨¡å‹

è¿‡å»ä¸€å¹´ï¼Œæˆ‘ä»¬è§è¯äº† OCR æ¨¡å‹é¢†åŸŸçš„çˆ†å‘å¼åˆ›æ–°ã€‚ç”±äºå¼€æºç”Ÿæ€çš„æ¨åŠ¨ï¼Œä¸åŒå›¢é˜Ÿä¹‹é—´å¯ä»¥ç›¸äº’å€Ÿé‰´ã€è¿­ä»£ï¼Œä»è€ŒåŠ é€Ÿäº†æŠ€æœ¯è¿›æ­¥ã€‚ä¸€ä¸ªå…¸å‹ä¾‹å­æ˜¯ AllenAI å‘å¸ƒçš„ **OlmOCR**ï¼Œå®ƒä¸ä»…å¼€æºäº†æ¨¡å‹æœ¬èº«ï¼Œè¿˜å…¬å¼€äº†è®­ç»ƒæ‰€ç”¨çš„æ•°æ®é›†ï¼Œä¸ºä»–äººæä¾›äº†å¯å¤ç°ä¸å¯æ‰©å±•çš„åŸºç¡€ã€‚
è¿™ä¸ªé¢†åŸŸæ­£ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦å‘å±•ï¼Œä½†å¦‚ä½•é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªä¸å°çš„æŒ‘æˆ˜ã€‚

---

### æœ€æ–°æ¨¡å‹å¯¹æ¯”

ä¸ºäº†å¸®åŠ©å¤§å®¶æ›´æ¸…æ™°åœ°äº†è§£å½“å‰æ ¼å±€ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å½“å‰ä¸»æµå¼€æº OCR æ¨¡å‹çš„éå®Œæ•´å¯¹æ¯”ã€‚
è¿™äº›æ¨¡å‹éƒ½å…·å¤‡ç‰ˆé¢ç†è§£èƒ½åŠ›ï¼ˆlayout-awareï¼‰ï¼Œèƒ½è§£æè¡¨æ ¼ã€å›¾è¡¨ä¸æ•°å­¦å…¬å¼ã€‚
å„æ¨¡å‹æ”¯æŒçš„è¯­è¨€èŒƒå›´å¯åœ¨å…¶ model card ä¸­æŸ¥çœ‹ã€‚é™¤ **Chandra**ï¼ˆOpenRAIL è®¸å¯ï¼‰ä¸ **Nanonets**ï¼ˆè®¸å¯è¯ä¸æ˜ï¼‰å¤–ï¼Œå…¶ä½™å‡ä¸ºå¼€æºè®¸å¯ã€‚

è¡¨æ ¼ä¸­å±•ç¤ºçš„å¹³å‡å¾—åˆ†æ¥è‡ª **Chandra** ä¸ **OlmOCR** æ¨¡å‹å¡ä¸­åœ¨ **OlmOCR Benchmark**ï¼ˆä»…è‹±æ–‡ï¼‰ä¸Šçš„æµ‹è¯•ç»“æœã€‚
æ­¤å¤–ï¼Œè®¸å¤šæ¨¡å‹åŸºäº **Qwen2.5-VL** æˆ– **Qwen3-VL** å¾®è°ƒï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿé™„ä¸Šäº† Qwen3-VL ä½œä¸ºå‚è€ƒã€‚

| æ¨¡å‹åç§°                                                                                                   | è¾“å‡ºæ ¼å¼                         | ç‰¹æ€§                                                       | æ¨¡å‹å¤§å° | æ˜¯å¦å¤šè¯­è¨€            | OlmOCR åŸºå‡†å¹³å‡å¾—åˆ† |
| :----------------------------------------------------------------------------------------------------- | :--------------------------- | :------------------------------------------------------- | :--- | :--------------- | :------------ |
| [Nanonets-OCR2-3B](https://huggingface.co/collections/nanonets/nanonets-ocr2-68ed207f17ee6c31d226319e) | ç»“æ„åŒ– Markdownï¼ˆå«è¯­ä¹‰æ ‡æ³¨ã€HTML è¡¨æ ¼ç­‰ï¼‰ | å›¾ç‰‡è‡ªåŠ¨ç”Ÿæˆè¯´æ˜<br>å¯æå–ç­¾åä¸æ°´å°<br>è¯†åˆ«å¤é€‰æ¡†ã€æµç¨‹å›¾ã€æ‰‹å†™ä½“                    | 4B   | âœ… è‹±è¯­ã€ä¸­æ–‡ã€æ³•è¯­ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ | N/A           |
| [PaddleOCR-VL](https://huggingface.co/collections/PaddlePaddle/paddleocr-vl-68f0db852483c7af0bc86849)  | Markdownã€JSONã€HTML è¡¨æ ¼ä¸å›¾è¡¨     | æ”¯æŒæ‰‹å†™ä½“ä¸æ—§æ–‡æ¡£<br>æ”¯æŒæç¤ºè¯è¾“å…¥<br>å¯å°†è¡¨æ ¼ä¸å›¾è¡¨è½¬æ¢ä¸º HTML<br>å¯ç›´æ¥æå–å¹¶æ’å…¥å›¾ç‰‡    | 0.9B | âœ… æ”¯æŒ 109 ç§è¯­è¨€     | N/A           |
| [dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)                                              | Markdownã€JSON                | æ”¯æŒ grounding<br>å¯æå–å¹¶æ’å…¥å›¾ç‰‡<br>æ”¯æŒæ‰‹å†™ä½“                        | 3B   | âœ… å¤šè¯­è¨€ï¼ˆå…·ä½“æœªè¯´æ˜ï¼‰     | 79.1 Â± 1.0    |
| [OlmOCR-2](https://huggingface.co/allenai/olmOCR-2-7B-1025)                                            | Markdownã€HTMLã€LaTeX          | å…·å¤‡ grounding èƒ½åŠ›<br>ä¼˜åŒ–äº†å¤§è§„æ¨¡æ‰¹å¤„ç†æ€§èƒ½                           | 8B   | â ä»…è‹±è¯­            | 82.3 Â± 1.1    |
| [Granite-Docling-258M](https://huggingface.co/ibm-granite/granite-docling-258M)                        | DocTags                      | æ”¯æŒåŸºäºæç¤ºçš„ä»»åŠ¡åˆ‡æ¢<br>å¯æŒ‡å®šå…ƒç´ ä½ç½®<br>è¾“å‡ºå†…å®¹ä¸°å¯Œ                         | 258M | âœ… è‹±è¯­ã€æ—¥è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡  | N/A           |
| [DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR)                                        | Markdownã€HTML                | æ”¯æŒé€šç”¨è§†è§‰ç†è§£<br>èƒ½å°†å›¾è¡¨ã€è¡¨æ ¼å®Œæ•´æ¸²æŸ“ä¸º HTML<br>è¯†åˆ«æ‰‹å†™ä½“<br>å†…å­˜é«˜æ•ˆï¼Œå›¾åƒæ–‡å­—è¯†åˆ«èƒ½åŠ›å¼º | 3B   | âœ… è¿‘ 100 ç§è¯­è¨€      | 75.4 Â± 1.0    |
| [Chandra](https://huggingface.co/datalab-to/chandra)                                                   | Markdownã€HTMLã€JSON           | å…·å¤‡ grounding èƒ½åŠ›<br>èƒ½åŸæ ·æå–å¹¶æ’å…¥å›¾ç‰‡                            | 9B   | âœ… æ”¯æŒ 40+ ç§è¯­è¨€     | 83.1 Â± 0.9    |
| [Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl)                                           | ä»»æ„æ ¼å¼è¾“å‡ºï¼ˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼‰              | è¯†åˆ«å¤æ–‡æ–‡æœ¬<br>æ”¯æŒæ‰‹å†™ä½“<br>å›¾ç‰‡å¯åŸæ ·æå–æ’å…¥                             | 9B   | âœ… æ”¯æŒ 32 ç§è¯­è¨€      | N/A           |

> **æ³¨ï¼š**
> Qwen3-VL æ˜¯ä¸€æ¬¾å¼ºå¤§çš„é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£ç†è§£ä»»åŠ¡ï¼Œä½†å¹¶æœªé’ˆå¯¹ OCR ä»»åŠ¡è¿›è¡Œç‰¹åˆ«ä¼˜åŒ–ã€‚
> å…¶ä»–æ¨¡å‹å¤šé‡‡ç”¨å›ºå®šæç¤ºè¯è¿›è¡Œå¾®è°ƒï¼Œä¸“ä¸º OCR ä»»åŠ¡è®¾è®¡ã€‚
> å› æ­¤è‹¥ä½¿ç”¨ Qwen3-VLï¼Œå»ºè®®å°è¯•ä¸åŒæç¤ºè¯ä»¥è·å¾—æ›´ä½³æ•ˆæœã€‚

ä½ å¯ä»¥é€šè¿‡è¿™ä¸ª [åœ¨çº¿æ¼”ç¤º](https://prithivMLmods-Multimodal-OCR3.hf.space) ä½“éªŒéƒ¨åˆ†æœ€æ–°æ¨¡å‹å¹¶æ¯”è¾ƒè¾“å‡ºæ•ˆæœï¼š

<iframe  
    src="https://prithivMLmods-Multimodal-OCR3.hf.space"  
    frameborder="0"  
    width="850"  
    height="450"  
></iframe>

---

### æ¨¡å‹è¯„ä¼°

#### åŸºå‡†æµ‹è¯•

æ²¡æœ‰ä»»ä½•ä¸€æ¬¾æ¨¡å‹èƒ½åœ¨æ‰€æœ‰åœºæ™¯ä¸­éƒ½æ˜¯â€œæœ€ä¼˜â€ã€‚
ä¾‹å¦‚ï¼šè¡¨æ ¼åº”ä»¥ Markdown è¿˜æ˜¯ HTML å‘ˆç°ï¼Ÿå“ªäº›å…ƒç´ éœ€è¦æå–ï¼Ÿå¦‚ä½•é‡åŒ–æ–‡æœ¬è¯†åˆ«å‡†ç¡®åº¦ï¼ŸğŸ‘€
è¿™äº›éƒ½å–å†³äºå…·ä½“ä»»åŠ¡ã€‚
ç›®å‰å·²æœ‰å¤šä¸ªå…¬å¼€è¯„æµ‹é›†ä¸å·¥å…·ï¼Œä½†ä»æ— æ³•è¦†ç›–æ‰€æœ‰æƒ…å†µã€‚
æˆ‘ä»¬æ¨èä»¥ä¸‹å¸¸ç”¨çš„è¯„æµ‹åŸºå‡†ï¼š

1. **[OmniDocBenchmark](https://huggingface.co/datasets/opendatalab/OmniDocBench)**

   * è¿™æ˜¯ç›®å‰ä½¿ç”¨æœ€å¹¿æ³›çš„æ–‡æ¡£è¯†åˆ«åŸºå‡†ä¹‹ä¸€ã€‚
   * è¦†ç›–æ–‡æ¡£ç±»å‹ä¸°å¯Œï¼šä¹¦ç±ã€æ‚å¿—ã€æ•™æç­‰ã€‚
   * æ”¯æŒå¤šæ ¼å¼ï¼ˆHTML ä¸ Markdownï¼‰è¡¨æ ¼è¯„æµ‹ã€‚
   * ä½¿ç”¨æ–°å‹ç®—æ³•è¯„ä¼°é˜…è¯»é¡ºåºï¼›å…¬å¼ä¼šåœ¨è¯„ä¼°å‰æ ‡å‡†åŒ–ã€‚
   * æŒ‡æ ‡åŸºäºâ€œç¼–è¾‘è·ç¦»â€æˆ–â€œæ ‘ç¼–è¾‘è·ç¦»â€ï¼ˆè¡¨æ ¼éƒ¨åˆ†ï¼‰ã€‚
   * æ ‡æ³¨æ•°æ®éƒ¨åˆ†ç”± SoTA VLM æˆ–ä¼ ç»Ÿ OCR ç”Ÿæˆã€‚

2. **[OlmOCR-Bench](https://huggingface.co/datasets/allenai/olmOCR-bench)**

   * é‡‡ç”¨â€œå•å…ƒæµ‹è¯•å¼â€è¯„ä¼°æ–¹å¼ã€‚
   * ä¾‹å¦‚ï¼šè¡¨æ ¼è¯„ä¼°é€šè¿‡éªŒè¯å•å…ƒæ ¼é—´å…³ç³»å®Œæˆã€‚
   * æ•°æ®æºä¸ºå…¬å¼€ PDFï¼Œæ ‡æ³¨æ¥è‡ªå¤šç§é—­æº VLMã€‚
   * ç‰¹åˆ«é€‚åˆè¯„ä¼°è‹±æ–‡ OCR æ¨¡å‹ã€‚

3. **[CC-OCR (Multilingual)](https://huggingface.co/datasets/wulipc/CC-OCR)**

   * ä¸å‰ä¸¤è€…ç›¸æ¯”ï¼ŒCC-OCR çš„æ–‡æ¡£è´¨é‡ä¸å¤šæ ·æ€§è¾ƒä½ã€‚
   * ä½†å®ƒæ˜¯**å”¯ä¸€**æ¶µç›–è‹±è¯­ä¸ä¸­æ–‡ä»¥å¤–è¯­è¨€çš„å¤šè¯­è¨€è¯„æµ‹é›†ã€‚
   * å›¾ç‰‡å¤šä¸ºä½è´¨é‡æ‹æ‘„ï¼Œæ–‡æœ¬è¾ƒå°‘ã€‚
   * å°½ç®¡ä¸å®Œç¾ï¼Œä½†ç›®å‰ä»æ˜¯æœ€ä½³çš„å¤šè¯­è¨€è¯„ä¼°é€‰é¡¹ã€‚

åœ¨ä¸åŒæ–‡æ¡£ç±»å‹ã€è¯­è¨€ä¸ä»»åŠ¡åœºæ™¯ä¸‹ï¼Œæ¨¡å‹è¡¨ç°å·®å¼‚æ˜æ˜¾ã€‚
å¦‚æœä½ çš„ä¸šåŠ¡é¢†åŸŸä¸åœ¨ç°æœ‰è¯„æµ‹é›†ä¸­ä½“ç°ï¼Œæˆ‘ä»¬å»ºè®®æ”¶é›†ä»£è¡¨æ€§æ ·æœ¬ï¼Œæ„å»ºè‡ªå®šä¹‰æµ‹è¯•é›†ï¼Œæ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨ä½ çš„ç‰¹å®šä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚

---

#### æˆæœ¬ä¸æ•ˆç‡

å¤§å¤šæ•° OCR æ¨¡å‹çš„è§„æ¨¡åœ¨ **3Bï½7B å‚æ•°**ä¹‹é—´ï¼Œä¹Ÿæœ‰ä¸€äº›å°å‹æ¨¡å‹ï¼ˆå¦‚ PaddleOCR-VL ä»… 0.9Bï¼‰ã€‚
æˆæœ¬ä¸ä»…ä¸æ¨¡å‹å¤§å°ç›¸å…³ï¼Œè¿˜å–å†³äºæ˜¯å¦æ”¯æŒé«˜æ•ˆæ¨ç†æ¡†æ¶ã€‚

ä¾‹å¦‚ï¼š

* **OlmOCR-2** æä¾› vLLM ä¸ SGLang å®ç°ã€‚

  * è‹¥åœ¨ H100 GPUï¼ˆ$2.69/å°æ—¶ï¼‰ä¸Šè¿è¡Œï¼Œæ¨ç†æˆæœ¬çº¦ä¸º **æ¯ç™¾ä¸‡é¡µ $178**ã€‚
* **DeepSeek-OCR** èƒ½åœ¨ä¸€å— 40GB A100 ä¸Šæ¯å¤©å¤„ç† **20 ä¸‡é¡µä»¥ä¸Š**ã€‚

  * ä»¥æ­¤ä¼°ç®—ï¼Œå…¶æˆæœ¬ä¸ OlmOCR å¤§è‡´ç›¸å½“ï¼ˆè§† GPU ä¾›åº”å•†è€Œå®šï¼‰ã€‚

è‹¥ä»»åŠ¡å¯¹ç²¾åº¦è¦æ±‚ä¸é«˜ï¼Œè¿˜å¯é€‰æ‹© **é‡åŒ–ç‰ˆæœ¬ï¼ˆQuantized Modelsï¼‰**ï¼Œè¿›ä¸€æ­¥é™ä½æˆæœ¬ã€‚
æ€»ä½“è€Œè¨€ï¼Œå¼€æºæ¨¡å‹åœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶å‡ ä¹æ€»æ¯”é—­æºæ–¹æ¡ˆæ›´ç»æµã€‚

---

#### å¼€æº OCR æ•°æ®é›†

å°½ç®¡è¿‘å¹´æ¥å¼€æº OCR æ¨¡å‹å¤§é‡æ¶Œç°ï¼Œä½†å…¬å¼€çš„è®­ç»ƒä¸è¯„æµ‹æ•°æ®é›†ä»ç›¸å¯¹ç¨€ç¼ºã€‚
ä¸€ä¸ªä¾‹å¤–æ˜¯ AllenAI çš„ [olmOCR-mix-0225](https://huggingface.co/datasets/allenai/olmOCR-mix-0225)ï¼Œ
æˆªè‡³ç›®å‰ï¼Œè¯¥æ•°æ®é›†å·²è¢«ç”¨äºè®­ç»ƒè‡³å°‘ [72 ä¸ªæ¨¡å‹](https://huggingface.co/models?dataset=dataset:allenai/olmOCR-mix-0225)ï¼ˆå¯èƒ½æ›´å¤šï¼‰ã€‚

æ›´å¹¿æ³›çš„æ•°æ®å…±äº«å°†æå¤§æ¨åŠ¨å¼€æº OCR çš„è¿›æ­¥ã€‚
ä»¥ä¸‹æ˜¯å‡ ç§å¸¸è§çš„æ•°æ®é›†æ„å»ºæ–¹å¼ï¼š

* **åˆæˆæ•°æ®ç”Ÿæˆï¼ˆSynthetic Data Generationï¼‰**
  ä¾‹å¦‚ï¼š[isl_synthetic_ocr](https://huggingface.co/datasets/Sigurdur/isl_synthetic_ocr)
* **VLM è‡ªåŠ¨è½¬å½•**ï¼Œå†ç»äººå·¥æˆ–å¯å‘å¼è¿‡æ»¤
* **åˆ©ç”¨ç°æœ‰ OCR æ¨¡å‹ç”Ÿæˆæ–°è®­ç»ƒæ•°æ®**ï¼Œä»¥è®­ç»ƒæ›´é«˜æ•ˆçš„é¢†åŸŸä¸“ç”¨æ¨¡å‹
* **åŸºäºäººå·¥æ ¡æ­£è¯­æ–™çš„å†åˆ©ç”¨**ï¼Œå¦‚ [è‹±å›½å°åº¦åŒ»å­¦å²æ•°æ®é›†](https://huggingface.co/NationalLibraryOfScotland)ï¼Œ
  å…¶ä¸­åŒ…å«å¤§é‡äººå·¥ä¿®æ­£çš„å†å²æ–‡æ¡£ OCR

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šæ­¤ç±»æ•°æ®é›†å·²å­˜åœ¨ä½†å°šæœªâ€œè®­ç»ƒåŒ–â€ï¼ˆtraining-readyï¼‰ã€‚
è‹¥èƒ½ç³»ç»ŸåŒ–æ•´ç†å¹¶å…¬å¼€ï¼Œå°†ä¸ºå¼€æºç¤¾åŒºé‡Šæ”¾å·¨å¤§æ½œåŠ›ã€‚

---

## æ¨¡å‹è¿è¡Œå·¥å…·

æˆ‘ä»¬æ”¶åˆ°è®¸å¤šå…³äºâ€œå¦‚ä½•å¼€å§‹ä½¿ç”¨ OCR æ¨¡å‹â€çš„é—®é¢˜ï¼Œå› æ­¤è¿™é‡Œæ€»ç»“äº†å‡ ç§ç®€å•çš„æ–¹å¼â€”â€”
åŒ…æ‹¬åœ¨æœ¬åœ°è¿è¡Œæ¨ç†ï¼Œæˆ–é€šè¿‡ Hugging Face è¿›è¡Œè¿œç¨‹æ‰˜ç®¡ã€‚

---

### æœ¬åœ°è¿è¡Œ

ç›®å‰å¤§å¤šæ•°å…ˆè¿› OCR æ¨¡å‹éƒ½æä¾› **vLLM** æ”¯æŒï¼Œå¹¶å¯é€šè¿‡ **transformers** åº“ç›´æ¥åŠ è½½æ¨ç†ã€‚
ä½ å¯ä»¥åœ¨å„æ¨¡å‹çš„ Hugging Face é¡µé¢æ‰¾åˆ°å…·ä½“è¯´æ˜ã€‚
ä¸‹é¢æˆ‘ä»¬ä»¥ **vLLM æ¨ç†æ–¹å¼**ä¸ºä¾‹æ¼”ç¤ºåŸºæœ¬æµç¨‹ã€‚

---

#### ä½¿ç”¨ vLLM å¯åŠ¨æœåŠ¡

```shell
vllm serve nanonets/Nanonets-OCR2-3B
```

ç„¶åï¼Œä½ å¯ä»¥é€šè¿‡ OpenAI SDK è¿›è¡Œè°ƒç”¨ï¼Œä¾‹å¦‚ï¼š

```py
from openai import OpenAI
import base64

client = OpenAI(base_url="http://localhost:8000/v1")
model = "nanonets/Nanonets-OCR2-3B"

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

def infer(img_base64):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},
                    },
                    {
                        "type": "text",
                        "text": "Extract the text from the above document as if you were reading it naturally.",
                    },
                ],
            }
        ],
        temperature=0.0,
        max_tokens=15000
    )
    return response.choices[0].message.content

img_base64 = encode_image(your_img_path)
print(infer(img_base64))
```

---

#### ä½¿ç”¨ Transformers è¿è¡Œæ¨ç†

Transformers åº“æä¾›äº†æ ‡å‡†åŒ–çš„æ¨¡å‹å®šä¹‰ä¸æ¥å£ï¼Œå¯è½»æ¾è¿›è¡Œæ¨ç†æˆ–å¾®è°ƒã€‚
æ¨¡å‹å¯èƒ½æœ‰ä¸¤ç§åŠ è½½æ–¹å¼ï¼š

1. **å®˜æ–¹å®ç°**ï¼ˆåœ¨ transformers å†…å®šä¹‰ï¼‰
2. **remote code å®ç°**ï¼ˆç”±æ¨¡å‹ä½œè€…å®šä¹‰ï¼Œå…è®¸ transformers è‡ªåŠ¨åŠ è½½ï¼‰

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ç”¨ transformers è°ƒç”¨ **Nanonets OCR æ¨¡å‹**ï¼š

```py
# å®‰è£…ä¾èµ–ï¼šflash-attn å’Œ transformers
from transformers import AutoProcessor, AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "nanonets/Nanonets-OCR2-3B", 
    torch_dtype="auto", 
    device_map="auto", 
    attn_implementation="flash_attention_2"
)
model.eval()
processor = AutoProcessor.from_pretrained("nanonets/Nanonets-OCR2-3B")

def infer(image_url, model, processor, max_new_tokens=4096):
    prompt = """Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using â˜ and â˜‘ for check boxes."""
    image = Image.open(image_path)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": [
            {"type": "image", "image": image_url},
            {"type": "text", "text": prompt},
        ]},
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt").to(model.device)
    
    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
    
    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return output_text[0]

result = infer(image_path, model, processor, max_new_tokens=15000)
print(result)
```

---

#### ä½¿ç”¨ MLXï¼ˆé€‚ç”¨äº Apple èŠ¯ç‰‡ï¼‰

**MLX** æ˜¯è‹¹æœæ¨å‡ºçš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸º **Apple Silicon (M ç³»åˆ—)** è®¾è®¡ã€‚
åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºçš„ [MLX-VLM](https://github.com/Blaizzy/mlx-vlm) èƒ½è½»æ¾è¿è¡Œè§†è§‰è¯­è¨€æ¨¡å‹ã€‚
ä½ å¯ä»¥åœ¨ [Hugging Face](https://huggingface.co/models?sort=trending&search=ocr) æœç´¢æ‰€æœ‰æ”¯æŒ MLX çš„ OCR æ¨¡å‹ï¼ˆåŒ…æ‹¬é‡åŒ–ç‰ˆæœ¬ï¼‰ã€‚

å®‰è£… MLX-VLMï¼š

```bash
pip install -U mlx-vlm
```

ç¤ºä¾‹è¿è¡Œï¼š

```bash
wget https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/throughput_smolvlm.png

python -m mlx_vlm.generate \
  --model ibm-granite/granite-docling-258M-mlx \
  --max-tokens 4096 \
  --temperature 0.0 \
  --prompt "Convert this chart to JSON." \
  --image throughput_smolvlm.png 
```

---

### è¿œç¨‹è¿è¡Œ

#### ä½¿ç”¨ Inference Endpoints éƒ¨ç½²æ¨¡å‹ï¼ˆæ‰˜ç®¡æ¨ç†æœåŠ¡ï¼‰

ä½ å¯ä»¥é€šè¿‡ **Hugging Face Inference Endpoints** åœ¨æ‰˜ç®¡ç¯å¢ƒä¸­éƒ¨ç½²å…¼å®¹ vLLM æˆ– SGLang çš„ OCR æ¨¡å‹ã€‚
è¯¥æœåŠ¡æä¾› GPU åŠ é€Ÿã€è‡ªåŠ¨ä¼¸ç¼©ã€ç›‘æ§ä¸å®‰å…¨æ‰˜ç®¡ï¼Œæ— éœ€è‡ªè¡Œç»´æŠ¤åŸºç¡€è®¾æ–½ã€‚

éƒ¨ç½²æ­¥éª¤å¦‚ä¸‹ï¼š

1. è¿›å…¥æ¨¡å‹ä»“åº“ [`nanonets/Nanonets-OCR2-3B`](https://huggingface.co/nanonets/Nanonets-OCR2-3B)

2. ç‚¹å‡»é¡µé¢ä¸Šçš„ **â€œDeployâ€** æŒ‰é’®ï¼Œé€‰æ‹© **â€œHF Inference Endpointsâ€**

   ![Inference Endpoints](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/IE.png)

3. åœ¨å¼¹å‡ºçš„çª—å£ä¸­é…ç½®éƒ¨ç½²å‚æ•°ï¼ˆGPU ç±»å‹ã€å®ä¾‹æ•°é‡ç­‰ï¼‰

   ![Inference Endpoints](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/IE2.png)

4. éƒ¨ç½²å®Œæˆåï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ä¸Šæ–‡ç¤ºä¾‹ä¸­çš„ OpenAI å®¢æˆ·ç«¯è„šæœ¬è°ƒç”¨è¯¥ Endpointã€‚

æ›´å¤šä¿¡æ¯å¯å‚é˜…å®˜æ–¹æ–‡æ¡£ï¼š
ğŸ‘‰ [Inference Endpoints (vLLM)](https://huggingface.co/docs/inference-endpoints/engines/vllm)

---

#### ä½¿ç”¨ Hugging Face Jobs è¿›è¡Œæ‰¹é‡æ¨ç†

å¯¹äº OCR åœºæ™¯ï¼Œå¾€å¾€éœ€è¦**æ‰¹é‡å¤„ç†æˆåƒä¸Šä¸‡å¼ å›¾åƒ**ã€‚
è¿™ç±»ä»»åŠ¡å¯é€šè¿‡ **vLLM çš„ç¦»çº¿æ¨ç†æ¨¡å¼** å®ç°é«˜æ•ˆå¹¶è¡Œã€‚

ä¸ºäº†ç®€åŒ–æµç¨‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº† [uv-scripts/ocr](https://huggingface.co/datasets/uv-scripts/ocr)ï¼Œ
å®ƒæ˜¯ä¸€ç»„é€‚é… Hugging Face Jobs çš„å¯ç›´æ¥è¿è¡Œè„šæœ¬ï¼Œèƒ½å®ç°ï¼š

* å¯¹æ•°æ®é›†åˆ—ä¸­çš„æ‰€æœ‰å›¾ç‰‡è¿›è¡Œæ‰¹é‡ OCR
* å°† OCR ç»“æœä»¥ Markdown å½¢å¼æ–°å¢ä¸ºæ–°åˆ—
* è‡ªåŠ¨å°†å¸¦ç»“æœçš„æ•°æ®é›†å›ä¼ åˆ° Hub

ä¾‹å¦‚ï¼Œå¤„ç† 100 å¼ å›¾ç‰‡çš„å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
hf jobs uv run --flavor l4x1 \
  https://huggingface.co/datasets/uv-scripts/ocr/raw/main/nanonets-ocr.py \
  your-input-dataset your-output-dataset \
  --max-samples 100
```

è¿™äº›è„šæœ¬ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰ vLLM é…ç½®ä¸æ‰¹æ¬¡æ¨ç†é€»è¾‘ï¼Œ
è®©æ‰¹é‡ OCR å˜å¾—æ— éœ€ GPU æˆ–å¤æ‚éƒ¨ç½²ã€‚

---

## è¶…è¶Š OCR

å¦‚æœä½ å¯¹æ–‡æ¡£æ™ºèƒ½ï¼ˆDocument AIï¼‰æ„Ÿå…´è¶£ï¼Œä¸ä»…ä»…å±€é™äºæ–‡å­—è¯†åˆ«ï¼ˆOCRï¼‰ï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„ä¸€äº›æ¨èæ–¹å‘ã€‚

---

### è§†è§‰æ–‡æ¡£æ£€ç´¢

**è§†è§‰æ–‡æ¡£æ£€ç´¢ï¼ˆVisual Document Retrievalï¼‰** æŒ‡çš„æ˜¯ï¼š
å½“ä½ è¾“å…¥ä¸€æ¡æ–‡æœ¬æŸ¥è¯¢æ—¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿä»å¤§é‡ PDF æ–‡æ¡£ä¸­ç›´æ¥æ£€ç´¢å‡ºæœ€ç›¸å…³çš„å‰ *k* ç¯‡ã€‚

ä¸ä¼ ç»Ÿæ–‡æœ¬æ£€ç´¢æ¨¡å‹ä¸åŒï¼Œè§†è§‰æ–‡æ¡£æ£€ç´¢å™¨ç›´æ¥åœ¨â€œæ–‡æ¡£å›¾åƒâ€å±‚é¢è¿›è¡Œæœç´¢ã€‚
é™¤äº†ç‹¬ç«‹ä½¿ç”¨å¤–ï¼Œä½ è¿˜å¯ä»¥å°†å®ƒä¸è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆï¼Œæ„å»º **å¤šæ¨¡æ€ RAGï¼ˆRetrieval-Augmented Generationï¼‰** ç®¡çº¿ã€‚
ç›¸å…³ç¤ºä¾‹å¯å‚è€ƒï¼š[ColPali + Qwen2_VL å¤šæ¨¡æ€ RAG æ•™ç¨‹](https://huggingface.co/merve/smol-vision/blob/main/ColPali_%2B_Qwen2_VL.ipynb)ã€‚

ä½ å¯ä»¥åœ¨ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=visual-document-retrieval&sort=trending) æ‰¾åˆ°æ‰€æœ‰å¯ç”¨çš„è§†è§‰æ–‡æ¡£æ£€ç´¢æ¨¡å‹ã€‚

ç›®å‰ä¸»æµçš„è§†è§‰æ£€ç´¢å™¨åˆ†ä¸ºä¸¤ç±»ï¼š

| ç±»å‹                              | ç‰¹ç‚¹                  | é€‚ç”¨åœºæ™¯         |
| ------------------------------- | ------------------- | ------------ |
| **å•å‘é‡æ¨¡å‹ï¼ˆSingle-vector Modelsï¼‰** | å†…å­˜æ•ˆç‡é«˜ã€é€Ÿåº¦å¿«ï¼Œä½†æ€§èƒ½ç•¥å¼±     | è½»é‡åŒ–éƒ¨ç½²ã€å¤§è§„æ¨¡ç´¢å¼•  |
| **å¤šå‘é‡æ¨¡å‹ï¼ˆMulti-vector Modelsï¼‰**  | è¡¨å¾èƒ½åŠ›å¼ºã€æ£€ç´¢ç²¾åº¦é«˜ï¼Œä½†å ç”¨æ˜¾å­˜æ›´å¤§ | é«˜ç²¾åº¦æ£€ç´¢ã€çŸ¥è¯†å¯†é›†ä»»åŠ¡ |

å¤§å¤šæ•°æ­¤ç±»æ¨¡å‹éƒ½æ”¯æŒ **vLLM** å’Œ **transformers**ï¼Œå› æ­¤ä½ å¯ä»¥å¾ˆæ–¹ä¾¿åœ°ç”¨å®ƒä»¬è¿›è¡Œå‘é‡ç´¢å¼•ï¼Œç„¶åç»“åˆå‘é‡æ•°æ®åº“ï¼ˆvector DBï¼‰æ‰§è¡Œé«˜æ•ˆæœç´¢ã€‚

---

### åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æ¡£é—®ç­”ï¼ˆDocument Question Answeringï¼‰

å¦‚æœä½ çš„ä»»åŠ¡ç›®æ ‡æ˜¯**åŸºäºæ–‡æ¡£å›ç­”é—®é¢˜**ï¼ˆè€Œä¸æ˜¯ä»…ä»…æå–æ–‡å­—ï¼‰ï¼Œ
ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ç»è¿‡æ–‡æ¡£ä»»åŠ¡è®­ç»ƒçš„**è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰**ã€‚

è®¸å¤šç”¨æˆ·ä¹ æƒ¯äºï¼š

1. å…ˆå°†æ–‡æ¡£è½¬æ¢æˆçº¯æ–‡æœ¬ï¼›
2. å†æŠŠæ–‡æœ¬ä¼ å…¥ LLM è¿›è¡Œé—®ç­”ã€‚

è¿™ç§æ–¹å¼è™½ç„¶å¯è¡Œï¼Œä½†å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼š

* ä¸€æ—¦æ–‡æ¡£å¸ƒå±€å¤æ‚ï¼ˆå¦‚å¤šæ ç»“æ„ã€å›¾è¡¨ã€å›¾ç‰‡è¯´æ˜ç­‰ï¼‰ï¼Œè½¬æ¢åçš„æ–‡æœ¬å°±å¯èƒ½ä¸¢å¤±å…³é”®ä¿¡æ¯ï¼›
* å›¾è¡¨è¢«è½¬ä¸º HTMLã€å›¾ç‰‡è¯´æ˜ç”Ÿæˆé”™è¯¯æ—¶ï¼ŒLLM å°±ä¼šè¯¯åˆ¤æˆ–å¿½ç•¥å†…å®¹ã€‚

å› æ­¤ï¼Œæ›´å¥½çš„åšæ³•æ˜¯ï¼š
ç›´æ¥å°†**åŸå§‹æ–‡æ¡£å›¾åƒ + ç”¨æˆ·é—®é¢˜** ä¸€èµ·è¾“å…¥æ”¯æŒå¤šæ¨¡æ€ç†è§£çš„æ¨¡å‹ï¼Œ
ä¾‹å¦‚ [Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe)ã€‚
è¿™æ ·æ¨¡å‹å°±èƒ½åŒæ—¶åˆ©ç”¨è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯ï¼Œä¸ä¼šé”™è¿‡ä»»ä½•ä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚

---

## æ€»ç»“

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¸ºä½ æ¦‚è§ˆäº†ç°ä»£ OCR æŠ€æœ¯çš„æ ¸å¿ƒè¦ç‚¹ï¼ŒåŒ…æ‹¬ï¼š

* å¦‚ä½•é€‰æ‹©åˆé€‚çš„ OCR æ¨¡å‹
* å½“å‰æœ€å‰æ²¿çš„å¼€æºæ¨¡å‹åŠå…¶èƒ½åŠ›
* åœ¨æœ¬åœ°æˆ–äº‘ç«¯è¿è¡Œæ¨¡å‹çš„å·¥å…·
* ä»¥åŠå¦‚ä½•åœ¨ OCR ä¹‹ä¸Šæ„å»ºæ›´å¤æ‚çš„æ–‡æ¡£æ™ºèƒ½åº”ç”¨

å¦‚æœä½ å¸Œæœ›è¿›ä¸€æ­¥æ·±å…¥äº†è§£ OCR ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œ
ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ¨èçš„å»¶ä¼¸é˜…è¯»ä¸æ•™ç¨‹èµ„æº ğŸ‘‡

---

### å»¶ä¼¸é˜…è¯»ä¸èµ„æº

* ğŸ“˜ [Vision Language Models Explainedï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹è¯¦è§£ï¼‰](https://huggingface.co/blog/vlms)
  â€”â€” æ·±å…¥ç†è§£ VLM çš„å·¥ä½œåŸç†ä¸å‘å±•å†ç¨‹ã€‚

* ğŸ§  [Vision Language Models 2025 Updateï¼ˆ2025 å¹´è§†è§‰è¯­è¨€æ¨¡å‹æ›´æ–°ï¼‰](https://huggingface.co/blog/vlms-2025)
  â€”â€” æœ€æ–° VLM æŠ€æœ¯è¿›å±•æ€»ç»“ã€‚

* ğŸ” [PP-OCR-v5 æŠ€æœ¯åšå®¢](https://huggingface.co/blog/baidu/ppocrv5)
  â€”â€” æ¥è‡ªç™¾åº¦çš„é«˜æ€§èƒ½ OCR ç³»ç»Ÿä¼˜åŒ–ä»‹ç»ã€‚

* ğŸ§© [æ•™ç¨‹ï¼šå¾®è°ƒ Kosmos2.5 è¿›è¡Œ Grounded OCR](https://huggingface.co/merve/smol-vision/blob/main/Grounded_Fine_tuning.ipynb)
  â€”â€” å®è·µæŒ‡å—ï¼Œæ•™ä½ å¦‚ä½•è®©æ¨¡å‹å…·å¤‡â€œé”šå®šå¼â€è¯†åˆ«èƒ½åŠ›ã€‚

* ğŸ“„ [æ•™ç¨‹ï¼šåœ¨ DocVQA æ•°æ®é›†ä¸Šå¾®è°ƒ Florence-2](https://huggingface.co/merve/smol-vision/blob/main/Fine_tune_Florence_2.ipynb)
  â€”â€” åŸºäºè§†è§‰é—®ç­”ä»»åŠ¡çš„å¾®è°ƒå®ä¾‹ã€‚

* ğŸ“± [åœ¨è®¾å¤‡ç«¯å®ç° SOTA OCRï¼ˆCore ML + dots.ocrï¼‰](https://huggingface.co/blog/dots-ocr-ne)
  â€”â€” å±•ç¤ºå¦‚ä½•åœ¨ç§»åŠ¨ç«¯é«˜æ•ˆéƒ¨ç½² OCR æ¨¡å‹ã€‚

---

**æ€»ç»“ä¸€å¥è¯ï¼š**
å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹æ­£åœ¨é‡æ–°å®šä¹‰ OCR çš„è¾¹ç•Œã€‚
ä»çº¯æ–‡æœ¬è¯†åˆ«åˆ°å¤šæ¨¡æ€ç†è§£ã€ä»å›¾åƒåˆ°è¯­ä¹‰ã€ä»ç¦»çº¿æ¨ç†åˆ°å¤§è§„æ¨¡éƒ¨ç½²â€”â€”
å¦‚ä»Šçš„å¼€æºç”Ÿæ€ä¸ºæ¯ä¸€ä¸ªå¼€å‘è€…å’Œç ”ç©¶è€…æä¾›äº†å‰æ‰€æœªæœ‰çš„è‡ªç”±åº¦ä¸åˆ›æ–°ç©ºé—´ã€‚

æ— è®ºä½ æ˜¯åœ¨æ„å»ºä¸‹ä¸€ä»£æ–‡æ¡£æ™ºèƒ½ç³»ç»Ÿï¼Œè¿˜æ˜¯ä»…æƒ³æ›´é«˜æ•ˆåœ°è§£æ PDFï¼Œ
å¸Œæœ›è¿™ç¯‡æŒ‡å—èƒ½å¸®åŠ©ä½ æ‰¾åˆ°æœ€åˆé€‚çš„èµ·ç‚¹ ğŸš€

---


# Supercharge your OCR Pipelines with Open Models

> [!TIP]
> We have added [Chandra](https://huggingface.co/datalab-to/chandra) and [OlmOCR-2](https://huggingface.co/allenai/olmOCR-2-7B-1025) to this blog, as well as OlmOCR Scores of the models ğŸ«¡


TL;DR: The rise of powerful vision-language models has transformed document AI. Each model comes with unique strengths, making it tricky to choose the right one. Open-weight models offer better cost efficiency and privacy. To help you get started with them, weâ€™ve put together this guide.

In this guide, youâ€™ll learn:

* The landscape of current models and their capabilities  
* When to fine-tune models vs. use models out-of-the-box  
* Key factors to consider when selecting a model for your use case  
* How to move beyond OCR with multimodal retrieval and document QA

By the end, youâ€™ll know how to choose the right OCR model, start building with it, and gain deeper insights into document AI. Letâ€™s go\!

## Table-of-Contents 

- [Supercharge your OCR Pipelines with Open Models](#supercharge-your-ocr-pipelines-with-open-models)
  - [Brief Introduction to Modern OCR](#brief-introduction-to-modern-ocr)
    - [Model Capabilities](#model-capabilities)
      - [Transcription](#transcription)
      - [Handling complex components in documents](#handling-complex-components-in-documents)
      - [Output formats](#output-formats)
      - [Locality Awareness in OCR](#locality-awareness-in-ocr)
      - [Model Prompting](#model-prompting)
  - [Cutting-edge Open OCR Models](#cutting-edge-open-ocr-models)
    - [Comparing Latest Models](#comparing-latest-models)
    - [Evaluating Models](#evaluating-models)
      - [Benchmarks](#benchmarks)
      - [Cost-efficiency](#cost-efficiency)
      - [Open OCR Datasets](#open-ocr-datasets)
  - [Tools to Run Models](#tools-to-run-models)
    - [Locally](#locally)
    - [Remotely](#remotely)
  - [Going Beyond OCR](#going-beyond-ocr)
    - [Visual Document Retrievers](#visual-document-retrievers)
    - [Using Vision Language Models for Document Question Answering](#using-vision-language-models-for-document-question-answering)
  - [Wrapping up](#wrapping-up)

## Brief Introduction to Modern OCR 

Optical Character Recognition (OCR) is one of the earliest and longest running challenges in computer vision.  Many of AIâ€™s first practical applications focused on turning printed text into digital form.

With the surge of [vision-language models](https://huggingface.co/blog/vlms) (VLMs), OCR has advanced significantly. Recently, many OCR models have been developed by fine-tuning existing VLMs. But todayâ€™s capabilities extend far beyond OCR: you can retrieve documents by query or answer questions about them directly. Thanks to stronger vision features, these models can also handle low-quality scans, interpret complex elements like tables, charts, and images, and fuse text with visuals to answer open-ended questions across documents.

### Model Capabilities

#### Transcription
Recent models transcribe texts into a machine-readable format.   
The input can include: 

- Handwritten text   
- Various scripts like Latin, Arabic, and Japanese characters  
- Mathematical expressions   
- Chemical formulas  
- Image/Layout/Page number tags

	  
OCR models convert them into machine-readable text that comes in many different formats like HTML, Markdown and more.  
	

#### Handling complex components in documents

On top of text, some models can also recognize:

- Images  
- Charts  
- Tables

Some models know where images are inside the document, extract their coordinates, and insert them appropriately between texts. Other models generate captions for images and insert them where they appear. This is especially useful if you are feeding the machine-readable output into an LLM. Example models are [OlmOCR by AllenAI](https://huggingface.co/allenai/olmOCR-7B-0825), or [PaddleOCR-VL by PaddlePaddle](https://huggingface.co/PaddlePaddle/PaddleOCR-VL).

Models use different machine-readable output formats, such as **DocTags**, **HTML** or **Markdown** (explained in the next section *Output Formats*). The way a model handles tables and charts often depends on the output format they are using. Some models treat charts like images: they are kept as is. Other models convert charts into markdown tables or JSON, e.g., a bar chart can be converted as follows. 

![Chart Rendering](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/chart-rendering.png)

Similarly for tables, cells are converted into a machine-readable format while retaining context from headings and columns. 

![Table Rendering](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/table-rendering.png)

#### Output formats
Different OCR models have different output formats. Briefly, here are the common output formats used by modern models.   
**DocTag:** DocTag is an XML-like format for documents that expresses location, text format, component-level information, and more. Below is an illustration of a paper parsed into DocTags. This format is employed by the open Docling models.  

![DocTags](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/doctags_v2.png)  

- **HTML:** HTML is one of the most popular output formats used for document parsing as it properly encodes structure and hierarchical information.   
- **Markdown:** Markdown is the most human-readable format. Itâ€™s simpler than HTML but not as expressive. For example, it canâ€™t represent split-column tables.  
- **JSON:** JSON is not a format that models use for the entire output, but it can be used to represent information in tables or charts.

The right model depends on how you plan to use its outputs:

* **Digital reconstruction**: To reconstruct documents digitally, choose a model with a layout-preserving format (e.g., DocTags or HTML).  
* **LLM input or Q\&A**: If the use case involves passing outputs to LLM, pick a model that outputs Markdown and image captions, since theyâ€™re closer to natural language.  
* **Programmatic use**: If you want to pass your outputs to a program (like data analysis), opt for a model that generates structured outputs like JSON.

#### Locality Awareness 

Documents can have complex structures, like multi-column text blocks and floating figures. Older OCR models handled these documents by detecting words and then the layout of pages manually in post-processing to have the text rendered in reading order, which is brittle.  Modern OCR models, on the other hand, incorporate layout metadata to help preserve reading order and accuracy. This metadata is called â€œanchorâ€, it can come in bounding boxes. This process is also called as â€œgrounding/anchoringâ€ because it helps with reducing hallucination.


#### Model Prompting

OCR models can either take in images and an optional text prompt, this depends on the model architecture and the pre-training setup.   
Some OCR models support prompt-based task switching, e.g. [granite-docling](https://huggingface.co/ibm-granite/granite-docling-258M) can parse an entire page with the prompt â€œConvert this page to Doclingâ€ while it can also take prompts like â€œConvert this formula to LaTeXâ€ along with a page full of formulas.   
Other models, however, are trained only for parsing entire pages, and they are conditioned to do this through a system prompt.   
For instance, [OlmOCR by AllenAI](https://huggingface.co/collections/allenai/olmocr-67af8630b0062a25bf1b54a1) takes a long conditioning prompt. Like many others, OlmOCR is technically an OCR fine-tuned version of a VLM (Qwen2.5VL in this case), so you can prompt for other tasks, but its performance will not be on par with the OCR capabilities. 

## Cutting-edge Open OCR Models

Weâ€™ve seen an incredible wave of new models this past year. Because so much work is happening in the open, these players build on and benefit from each otherâ€™s work. A great example is AllenAIâ€™s release of OlmOCR, which not only released a model but also the dataset used to train it. With these, others can build upon them in new directions. The field is incredibly active, but itâ€™s not always obvious which model to use. 

### Comparing Latest Models

To make things a bit easier, weâ€™re putting together a non-exhaustive comparison of some of our current favorite models. All of the models below are layout-aware and can parse tables, charts, and math equations. The full list of languages each model supports are detailed in their model cards, so make sure to check them if youâ€™re interested. All models below have open-source license except for Chandra having OpenRAIL license and Nanonets license being unclear. The average scores are taken from model cards of Chandra, OlmOCR, evaluated on OlmOCR Benchmark, which is English-only.
Many of the models in this collection have been fine-tuned from Qwen2.5-VL or Qwen3-VL, so we also provide Qwen3-VL model below as well. 

| Model Name | Output formats | Features | Model Size | Multilingual? | Average Score on OlmOCR Benchmark |
| :---- | :---- | :---- | :---- | :---- | :---- |
| [Nanonets-OCR2-3B](https://huggingface.co/collections/nanonets/nanonets-ocr2-68ed207f17ee6c31d226319e) | structured Markdown with semantic tagging (plus HTML tables, etc.) | Captions images in the documents<br>Signature & watermark extraction<br>Handles checkboxes, flowcharts, and handwriting | 4B | âœ…Supports English, Chinese, French, Arabic and more. | N/A |
| [PaddleOCR-VL](https://huggingface.co/collections/PaddlePaddle/paddleocr-vl-68f0db852483c7af0bc86849) | Markdown, JSON, HTML tables and charts | Handles handwriting, old documents<br>Allows prompting<br>Converts tables & charts to HTML<br>Extracts and inserts images directly | 0.9B | âœ…Supports 109 languages | N/A |
| [dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr) | Markdown, JSON | Grounding<br>Extracts and inserts images<br>Handles handwriting | 3B | âœ…Multilingual with language info not available | 79.1 Â± 1.0 |
| [OlmOCR-2](https://huggingface.co/allenai/olmOCR-2-7B-1025) | Markdown, HTML, LaTeX | Grounding<br>Optimized for large-scale batch processing | 8B | âEnglish-only | 82.3 Â± 1.1 |
| [Granite-Docling-258M](https://huggingface.co/ibm-granite/granite-docling-258M) | DocTags | Prompt-based task switching<br>Ability to prompt element locations with location tokens<br>Rich output | 258M | âœ…Supports English, Japanese, Arabic and Chinese. | N/A | 
| [DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR) | Markdown, HTML | Supports general visual understanding<br>Can parse and re-render all charts, tables, and more into HTML<br>Handles handwriting<br>Memory-efficient, solves text through image | 3B | âœ…Supports nearly 100 languages | 75.4 Â± 1.0 |
| [Chandra](https://huggingface.co/deepseek-ai/DeepSeek-OCR) | Markdown, HTML, JSON | Grounding<br>Extracts and inserts images as is | 9B | âœ…Supports 40+ languages | 83.1 Â± 0.9 |
| [Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl) | Vision Language Model can output in all formats | Can recognize ancient text<br>Handles handwriting<br>Extracts and inserts images as is | 9B | âœ…Supports 32 languages | N/A |

While Qwen3-VL itself is a powerful and versatile vision-language model post-trained for document understanding and other tasks, it isnâ€™t optimized for a single, universal OCR prompt. In contrast, the other models were fine-tuned using one or a few fixed prompts specifically designed for OCR tasks. So to use Qwen3-VL, we recommend experimenting with prompts.

Hereâ€™s a [small demo](https://prithivMLmods-Multimodal-OCR3.hf.space) for you to try some of the latest models and compare their outputs.   
<iframe  
    src="https://prithivMLmods-Multimodal-OCR3.hf.space"  
    frameborder="0"  
    width="850"  
    height="450"

></iframe>

### Evaluating Models

#### Benchmarks

Thereâ€™s no single best model, as every problem has different needs. Should tables be rendered in Markdown or HTML? Which elements should we extract? How should we quantify text accuracy and error rates? ğŸ‘€  
While there are many evaluation datasets and tools, many donâ€™t answer these questions. So we suggest using the following benchmarks:

1. [**OmniDocBenchmark**](https://huggingface.co/datasets/opendatalab/OmniDocBench)**:** This widely used benchmark stands out for its diverse document types: books, magazines, and textbooks. Its evaluation criteria are well designed, accepting tables in both HTML and Markdown formats. A novel matching algorithm evaluates the reading order, and formulas are normalized before evaluation. Most metrics rely on edit distance or tree edit distance (tables). Notably, the annotations used for evaluation are not solely human-generated but are acquired through SoTA VLMs or conventional OCR methods.  
2. [**OlmOCR-Bench**](https://huggingface.co/datasets/allenai/olmOCR-bench): OlmOCR-Bench takes a different approach: they treat the evaluation as a set of unit tests. For example, table evaluation is done by checking the relation between selected cells of a given table. They use PDFs from public sources, and annotations are done using a wide range of closed-source VLMs. This benchmark is quite successful to evaluate on the English language.  
3. [**CC-OCR (Multilingual)**:](https://huggingface.co/datasets/wulipc/CC-OCR) Compared to the previous benchmarks, CC-OCR is less preferred when picking models, due to lower document quality and diversity. However, itâ€™s the only benchmark that contains evaluation beyond English and Chinese\! While the evaluation is far from perfect (images are photos with few words), itâ€™s still the best you can do for multilingual evaluation.

When testing different OCR models, we've found that the performance across different document types, languages, etc., varies a lot. Your domain may not be well represented in existing benchmarks\! To make effective use of this new generation of VLM-based OCR models we suggest aiming to collect a dataset of representative examples of your task domain and testing a few different models to compare their performance. 

#### Cost-efficiency

Most OCR models are small, having between 3B and 7B parameters; you can even find models with fewer than 1B parameters, like PaddleOCR-VL. However, the cost also depends on the availability of optimized implementations for specialized inference frameworks. For example, OlmOCR-2 comes with vLLM and SGLang implementations, and the cost per million pages is 178 dollars (assuming on H100 for $2.69/hour). DeepSeek-OCR can process 200k+ pages per day on a single A100 with 40GB VRAM.  With napkin math, we see that the cost per million pages is more or less similar to OlmOCR (although it depends on your A100 provider). If your use case remains unaffected, you can also opt for quantized versions of the models. The cost of running open-source models heavily depends on the hourly cost of the instance and the optimizations the model includes, but itâ€™s guaranteed to be cheaper than many closed-source models out there on a larger scale.

#### Open OCR Datasets 

While the past year has seen a surge in open OCR models, this hasn't been matched by as many open training and evaluation datasets. An exception is AllenAI's [olmOCR-mix-0225](https://huggingface.co/datasets/allenai/olmOCR-mix-0225), which has been used to train at least [72 models on the Hub](https://huggingface.co/models?dataset=dataset:allenai/olmOCR-mix-0225) â€“ likely more, since not all models document their training data.

Sharing more datasets could unlock even greater advances in open OCR models. There are several promising approaches for creating these datasets:

- **Synthetic data generation** (e.g., [isl_synthetic_ocr](https://huggingface.co/datasets/Sigurdur/isl_synthetic_ocr))  
- **VLM-generated transcriptions** filtered manually or through heuristics  
- **Using existing OCR models** to generate training data for new, potentially more efficient models in specific domains  
- **Leveraging existing corrected datasets** like the [Medical History of British India Dataset](https://huggingface.co/NationalLibraryOfScotland), which contains extensively human-corrected OCR for historic documents

It's worth noting that many such datasets exist but remain unused. Making them more readily available as 'training-ready' datasets carries a considerable potential for the open-source community.

## Tools to Run Models

We have received many questions about getting started with OCR models, so here are a few ways you can use local inference tools and host remotely with Hugging Face.

### Locally

Most cutting-edge models come with vLLM support and transformers implementation. You can get more info about how to serve each from the modelsâ€™ own cards. For convenience, we show how to infer locally using vLLM here. The code below can differ from model to model, but for most models it looks like the following. 

```shell
vllm serve nanonets/Nanonets-OCR2-3B
```

And then you can query as follows using e.g. OpenAI client. 

```py
from openai import OpenAI
import base64

client = OpenAI(base_url="http://localhost:8000/v1")

model = "nanonets/Nanonets-OCR2-3B"

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

def infer(img_base64):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},
                    },
                    {
                        "type": "text",
                        "text": "Extract the text from the above document as if you were reading it naturally.",
                    },
                ],
            }
        ],
        temperature=0.0,
        max_tokens=15000
    )
    return response.choices[0].message.content

img_base64 = encode_image(your_img_path)
print(infer(img_base64))
```

**Transformers**

Transformers provides standard model definitions for easy inference and fine-tuning. Models available in transformers come with either official transformers implementation (model definitions within the library) or â€œremote codeâ€ implementations. Latter is defined by the model owners to enable easy loading of models into transformers interface, so you donâ€™t have to go through the model implementation. Below is an example loading Nanonets model using transformers implementation.

```py
# make sure to install flash-attn and transformers
from transformers import AutoProcessor, AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "nanonets/Nanonets-OCR2-3B", 
    torch_dtype="auto", 
    device_map="auto", 
    attn_implementation="flash_attention_2"
)
model.eval()
processor = AutoProcessor.from_pretrained("nanonets/Nanonets-OCR2-3B")

def infer(image_url, model, processor, max_new_tokens=4096):
    prompt = """Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using â˜ and â˜‘ for check boxes."""
    image = Image.open(image_path)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": [
            {"type": "image", "image": image_url},
            {"type": "text", "text": prompt},
        ]},
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt").to(model.device)
    
    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
    
    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return output_text[0]

result = infer(image_path, model, processor, max_new_tokens=15000)
print(result)
```

**MLX**  
MLX is an open-source machine learning framework for Apple Silicon. [MLX-VLM](https://github.com/Blaizzy/mlx-vlm) is built on top of MLX to serve vision language models easily. You can explore all the OCR models available in MLX format [here](https://huggingface.co/models?sort=trending&search=ocr). They also come in quantized versions.  
You can install MLX-VLM as follows.

```
pip install -U mlx-vlm
```

```
wget https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/throughput_smolvlm.png

python -m mlx_vlm.generate --model ibm-granite/granite-docling-258M-mlx --max-tokens 4096 --temperature 0.0 --prompt "Convert this chart to JSON." --image throughput_smolvlm.png 

```

### Remotely

**Inference Endpoints for Managed Deployment**  
You can deploy OCR models compatible with vLLM or SGLang on Hugging Face Inference Endpoints, either from a model repository â€œDeployâ€ option or directly through [Inference Endpoints interface](https://endpoints.huggingface.co/). Inference Endpoints serve the cutting-edge models in a fully managed environment with GPU acceleration, auto-scaling, and monitoring without manually managing the infrastructure.  
   
Here is a simple method of deploying `nanonets` using vLLM as the inference engine.

1. Navigate to the model repository [`nanonets/Nanonets-OCR2-3B`](https://huggingface.co/nanonets/Nanonets-OCR2-3B)  
2. Click on the â€œDeployâ€ button and select the â€œHF Inference Endpointsâ€

![Inference Endpoints](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/IE.png)

3. Configure the deployment setup within seconds

![Inference Endpoints](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ocr/IE2.png)

4. After the endpoint is created, you can consume it using the OpenAI client snippet we provided in the previous section.

You can learn more about it [here](https://huggingface.co/docs/inference-endpoints/engines/vllm).

**Hugging Face Jobs for Batch Inference** 

For many OCR applications, you want to do efficient batch inference, i.e., running a model across thousands of images as cheaply and efficiently as possible. A good approach is to use vLLM's offline inference mode. As discussed above, many recent VLM-based OCR models are supported by vLLM, which efficiently batches images and generates OCR outputs at scale.

To make this even easier, we've created [uv-scripts/ocr](https://huggingface.co/datasets/uv-scripts/ocr), a collection of ready-to-run OCR scripts that work with Hugging Face Jobs. These scripts let you run OCR on any dataset without needing your own GPU. Simply point the script at your input dataset, and it will:

- Process all images in a dataset column using many different open OCR models  
- Add OCR results as a new markdown column to the dataset  
- Push the updated dataset with OCR results to the Hub

For example, to run OCR on 100 images:

```bash  
hf jobs uv run --flavor l4x1 \
  https://huggingface.co/datasets/uv-scripts/ocr/raw/main/nanonets-ocr.py \
  your-input-dataset your-output-dataset \
  --max-samples 100
```

The scripts handle all the vLLM configuration and batching automatically, making batch OCR accessible without infrastructure setup.

### Going Beyond OCR

If you are interested in document AI, not just OCR, here are some of our recommendations. 

#### Visual Document Retrievers
Visual document retrieval is to retrieve the most relevant top-k documents when given a text query. If you have previously worked with retriever models, the difference is that you search directly on a stack of PDFs. Aside from using them standalone, you can also build multimodal RAG pipelines by combining them with a vision language model (find how to do so [here](https://huggingface.co/merve/smol-vision/blob/main/ColPali\_%2B\_Qwen2\_VL.ipynb)). You can find [all of them on Hugging Face Hub](https://huggingface.co/models?pipeline\_tag=visual-document-retrieval\&sort=trending).

There are two types of visual document retrievers, single-vector and multi-vector models. Single-vector models are more memory efficient and less performant; meanwhile, multi-vector models are more memory hungry and more performant. Most of these models often come with vLLM and transformers integrations, so you can index documents using them and then do a search easily using a vector DB.

#### Using Vision Language Models for Document Question Answering
If you have a task at hand that only requires answering questions based on documents, you can use some of the vision language models that had document tasks in their training tasks. Weâ€™ve observed users trying to convert documents into text and passing the output to LLMs, but if your document has a complex layout, and your converted document outputs charts and so on in HTML, or images are captioned incorrectly, the LLM will miss out. Instead, feed your document and query to one of the advanced vision language models like [Qwen3-VL](https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe) not to miss out on any context. 

## Wrapping up

In this blog post, we wanted to give you an overview of how to pick your OCR model, existing cutting-edge models and capabilities, and the tools to get you started with OCR.   
If you want to learn more about OCR and vision language models, we encourage you to read the resources below. 

- [Vision Language Models Explained](https://huggingface.co/blog/vlms)  
- [Vision Language Models 2025 Update](https://huggingface.co/blog/vlms-2025)  
- [Blog on PP-OCR-v5](https://huggingface.co/blog/baidu/ppocrv5)
- [Tutorial: Fine-tuning Kosmos2.5 on Grounded OCR](https://huggingface.co/merve/smol-vision/blob/main/Grounded_Fine_tuning.ipynb)
- [Tutorial: Fine-tuning Florence-2 on DocVQA](https://huggingface.co/merve/smol-vision/blob/main/Fine_tune_Florence_2.ipynb)
- [SOTA OCR on-device with Core ML and dots.ocr](https://huggingface.co/blog/dots-ocr-ne)

